{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f84a0faf",
      "metadata": {},
      "source": [
        "#### 필수 라이브러리 설치"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "354b1e04",
      "metadata": {
        "id": "354b1e04"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain langchain-openai langchain_community langchain_experimental langchain-text-splitters tiktoken pypdf faiss-cpu wikipedia"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1eedff44",
      "metadata": {},
      "source": [
        "#### OpenAI API Key 등록"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8U_lhAypNH9G",
      "metadata": {
        "id": "8U_lhAypNH9G"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = 'sk-*********************************************************'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fa23b62",
      "metadata": {
        "id": "9fa23b62"
      },
      "source": [
        "# Conversational Agent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1Ol7DzUh-Ung",
      "metadata": {
        "id": "1Ol7DzUh-Ung"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from pydantic import BaseModel, Field\n",
        "import datetime\n",
        "from langchain.tools import tool\n",
        "\n",
        "\n",
        "#함수를 tool로 변환\n",
        "\n",
        "@tool\n",
        "def get_current_temperature(latitude: float, longitude: float) -> dict:\n",
        "    \"\"\"Fetch current temperature for given coordinates.\"\"\"\n",
        "\n",
        "    BASE_URL = \"https://api.open-meteo.com/v1/forecast\"\n",
        "\n",
        "    # Parameters for the request\n",
        "    params = {\n",
        "        'latitude': latitude,\n",
        "        'longitude': longitude,\n",
        "        'hourly': 'temperature_2m',\n",
        "        'forecast_days': 1,\n",
        "    }\n",
        "\n",
        "    # Make the request\n",
        "    response = requests.get(BASE_URL, params=params)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        results = response.json()\n",
        "    else:\n",
        "        raise Exception(f\"API Request failed with status code: {response.status_code}\")\n",
        "\n",
        "    current_utc_time = datetime.datetime.utcnow()\n",
        "    time_list = [datetime.datetime.fromisoformat(time_str.replace('Z', '+00:00')) for time_str in results['hourly']['time']]\n",
        "    temperature_list = results['hourly']['temperature_2m']\n",
        "\n",
        "    closest_time_index = min(range(len(time_list)), key=lambda i: abs(time_list[i] - current_utc_time))\n",
        "    current_temperature = temperature_list[closest_time_index]\n",
        "\n",
        "    return f'The current temperature is {current_temperature}°C'\n",
        "\n",
        "import wikipedia\n",
        "\n",
        "@tool\n",
        "def search_wikipedia(query: str) -> str:\n",
        "    \"\"\"Run Wikipedia search and get page summaries.\"\"\"\n",
        "    page_titles = wikipedia.search(query)\n",
        "    summaries = []\n",
        "    for page_title in page_titles[: 3]:\n",
        "        try:\n",
        "            wiki_page =  wikipedia.page(title=page_title, auto_suggest=False)\n",
        "            summaries.append(f\"Page: {page_title}\\nSummary: {wiki_page.summary}\")\n",
        "        except (\n",
        "            self.wiki_client.exceptions.PageError,\n",
        "            self.wiki_client.exceptions.DisambiguationError,\n",
        "        ):\n",
        "            pass\n",
        "    if not summaries:\n",
        "        return \"No good Wikipedia Search Result was found\"\n",
        "    return \"\\n\\n\".join(summaries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "q3YsjtWFcLW1",
      "metadata": {
        "id": "q3YsjtWFcLW1"
      },
      "outputs": [],
      "source": [
        "#tool을 리스트로 반환\n",
        "tools = [get_current_temperature, search_wikipedia]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5YgubNnacPxd",
      "metadata": {
        "id": "5YgubNnacPxd"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.tools.render import format_tool_to_openai_function\n",
        "from langchain.agents.format_scratchpad import format_to_openai_functions\n",
        "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "93ffsBoRcUY8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93ffsBoRcUY8",
        "outputId": "6cd45050-ce24-489b-8b2e-17403251272c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `format_tool_to_openai_function` was deprecated in LangChain 0.1.16 and will be removed in 0.2.0. Use langchain_core.utils.function_calling.convert_to_openai_function() instead.\n",
            "  warn_deprecated(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import MessagesPlaceholder\n",
        "\n",
        "#tool을 openai function으로 포맷 변경\n",
        "functions = [format_tool_to_openai_function(f) for f in tools]\n",
        "\n",
        "#함수를 가진 모델 생성\n",
        "model = ChatOpenAI(temperature=0).bind(functions=functions)\n",
        "\n",
        "#프롬프트 생성 \n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are helpful but sassy assistant\"),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"user\", \"{input}\"),\n",
        "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
        "])\n",
        "\n",
        "#LCEL 체인 생성\n",
        "chain = prompt | model | OpenAIFunctionsAgentOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "6C5KqfTPdahT",
      "metadata": {
        "id": "6C5KqfTPdahT"
      },
      "outputs": [],
      "source": [
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "#체인 생성 \n",
        "agent_chain = RunnablePassthrough.assign(\n",
        "    #agent_scratchpad 변수에 OPENAI Function 내부 값을 입력\n",
        "    agent_scratchpad= lambda x: format_to_openai_functions(x[\"intermediate_steps\"])\n",
        ") | prompt | model | OpenAIFunctionsAgentOutputParser() #OpenAI Function Agent 의 Output Parser 적용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "GGhSWBmYdG-c",
      "metadata": {
        "id": "GGhSWBmYdG-c"
      },
      "outputs": [],
      "source": [
        "from langchain.schema.agent import AgentFinish\n",
        "\n",
        "#AGENT 생성\n",
        "def run_agent(user_input):\n",
        "    intermediate_steps = []\n",
        "    while True:\n",
        "        #intermediate_steps 값이 있으면 result 반환\n",
        "        result = agent_chain.invoke({\n",
        "            \"input\": user_input,\n",
        "            \"intermediate_steps\": intermediate_steps\n",
        "        })\n",
        "        if isinstance(result, AgentFinish):\n",
        "            return result\n",
        "        \n",
        "        #없으면 tool 설정\n",
        "        tool = {\n",
        "            \"search_wikipedia\": search_wikipedia,\n",
        "            \"get_current_temperature\": get_current_temperature,\n",
        "        }[result.tool]\n",
        "\n",
        "        #tool 실행\n",
        "        observation = tool.run(result.tool_input)\n",
        "        intermediate_steps.append((result, observation))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "qxWXYRR2d1Js",
      "metadata": {
        "id": "qxWXYRR2d1Js"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "#Chat Memory 생성\n",
        "memory = ConversationBufferMemory(return_messages=True,memory_key=\"chat_history\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "weckDQVkdak7",
      "metadata": {
        "id": "weckDQVkdak7"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import AgentExecutor\n",
        "\n",
        "#Agent 실행기 생성\n",
        "agent_executor = AgentExecutor(agent=agent_chain, tools=tools, verbose=True, memory=memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "23FOWDkadeHk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23FOWDkadeHk",
        "outputId": "330d65bd-a4b1-4fb9-9f61-2251a3a60a58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `search_wikipedia` with `{'query': 'Langchain'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3mPage: LangChain\n",
            "Summary: LangChain is a framework designed to simplify the creation of applications using large language models (LLMs). As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\n",
            "\n",
            "\n",
            "\n",
            "Page: OpenAI\n",
            "Summary: OpenAI is a U.S. based artificial intelligence (AI) research organization founded in December 2015, researching artificial intelligence with the goal of developing \"safe and beneficial\" artificial general intelligence, which it defines as \"highly autonomous systems that outperform humans at most economically valuable work\".\n",
            "As one of the leading organizations of the AI spring, it has developed several large language models, advanced image generation models, and previously, released open-source models. Its release of ChatGPT has been credited with starting the AI spring.The organization consists of the non-profit OpenAI, Inc. registered in Delaware and its for-profit subsidiary OpenAI Global, LLC. It was founded by Ilya Sutskever, Greg Brockman, Trevor Blackwell, Vicki Cheung, Andrej Karpathy, Durk Kingma, Jessica Livingston, John Schulman, Pamela Vagata, and Wojciech Zaremba, with Sam Altman and Elon Musk serving as the initial Board of Directors members. Microsoft provided OpenAI Global LLC with a $1 billion investment in 2019 and a $10 billion investment in 2023, with a significant portion of the investment in the form of computational resources on Microsoft's Azure cloud service.On November 17, 2023, the board removed Altman as CEO, while Brockman was removed as chairman and then resigned as president. Four days later, both returned after negotiations with the board, and most of the board members resigned. The new initial board included former Salesforce co-CEO Bret Taylor as chairman. It was also announced that Microsoft will have a non-voting board seat.\n",
            "\n",
            "\n",
            "\n",
            "Page: DataStax\n",
            "Summary: DataStax, Inc. is a real-time data for AI company based in Santa Clara, California. Its product Astra DB is a cloud database-as-a-service based on Apache Cassandra. DataStax also offers DataStax Enterprise (DSE), an on-premises database built on Apache Cassandra, and Astra Streaming, a messaging and event streaming cloud service based on Apache Pulsar. As of June 2022, the company has roughly 800 customers distributed in over 50 countries.\u001b[0m\u001b[32;1m\u001b[1;3mLangChain is a framework designed to simplify the creation of applications using large language models (LLMs). It is a language model integration framework that can be used for document analysis and summarization, chatbots, and code analysis.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'what is langchain?',\n",
              " 'chat_history': [HumanMessage(content='what is langchain?'),\n",
              "  AIMessage(content='LangChain is a framework designed to simplify the creation of applications using large language models (LLMs). It is a language model integration framework that can be used for document analysis and summarization, chatbots, and code analysis.')],\n",
              " 'output': 'LangChain is a framework designed to simplify the creation of applications using large language models (LLMs). It is a language model integration framework that can be used for document analysis and summarization, chatbots, and code analysis.'}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#wiki 함수 작동 실행\n",
        "agent_executor.invoke({\"input\": \"what is langchain?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "5QMETToHd_3b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QMETToHd_3b",
        "outputId": "ea9749a1-5b4d-4eb6-d062-8e048b22d211"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `get_current_temperature` with `{'latitude': 37.7749, 'longitude': -122.4194}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3mThe current temperature is 13.4°C\u001b[0m\u001b[32;1m\u001b[1;3mThe current temperature in San Francisco is 13.4°C.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'whats the weather in sf?',\n",
              " 'chat_history': [HumanMessage(content='what is langchain?'),\n",
              "  AIMessage(content='LangChain is a framework designed to simplify the creation of applications using large language models (LLMs). It is a language model integration framework that can be used for document analysis and summarization, chatbots, and code analysis.'),\n",
              "  HumanMessage(content='whats the weather in sf?'),\n",
              "  AIMessage(content='The current temperature in San Francisco is 13.4°C.')],\n",
              " 'output': 'The current temperature in San Francisco is 13.4°C.'}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#weather 함수 작동 실행 \n",
        "agent_executor.invoke({\"input\": \"whats the weather in sf?\"})"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
