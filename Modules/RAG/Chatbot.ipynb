{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Set up"
      ],
      "metadata": {
        "id": "IgHeElS0UAXP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_f4kdIaV5GtR"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --quiet langchain langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = 'sk-********************************************************'"
      ],
      "metadata": {
        "id": "Xi78Gmm85ceP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Memory management"
      ],
      "metadata": {
        "id": "rCXF7EisO1WN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Message passing"
      ],
      "metadata": {
        "id": "FJ28kKZ9PoBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chat = ChatOpenAI(model=\"gpt-3.5-turbo-1106\")"
      ],
      "metadata": {
        "id": "D3EZRMpC6Bz8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt | chat\n",
        "\n",
        "chain.invoke(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            HumanMessage(\n",
        "                content=\"Translate this sentence from English to French: I love programming.\"\n",
        "            ),\n",
        "            AIMessage(content=\"J'adore la programmation.\"),\n",
        "            HumanMessage(content=\"What did you just say?\"),\n",
        "        ],\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IS03RO5GO4bU",
        "outputId": "7222e564-4402-440f-d3b2-dd437559a61b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='I said \"J\\'adore la programmation,\" which means \"I love programming\" in French.', response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 61, 'total_tokens': 82}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': 'fp_9dd82289bf', 'finish_reason': 'stop', 'logprobs': None})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat history"
      ],
      "metadata": {
        "id": "4eHZy6_ZPsji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ChatMessageHistory\n",
        "\n",
        "demo_ephemeral_chat_history = ChatMessageHistory()\n",
        "\n",
        "demo_ephemeral_chat_history.add_user_message(\n",
        "    \"Translate this sentence from English to French: I love programming.\"\n",
        ")\n",
        "\n",
        "demo_ephemeral_chat_history.add_ai_message(\"J'adore la programmation.\")\n",
        "\n",
        "demo_ephemeral_chat_history.messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hap6BgFMPQ3b",
        "outputId": "797f1c2d-e935-450a-9a0d-60d2c59c6d13"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='Translate this sentence from English to French: I love programming.'),\n",
              " AIMessage(content=\"J'adore la programmation.\")]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demo_ephemeral_chat_history = ChatMessageHistory()\n",
        "\n",
        "input1 = \"Translate this sentence from English to French: I love programming.\"\n",
        "\n",
        "demo_ephemeral_chat_history.add_user_message(input1)\n",
        "\n",
        "response = chain.invoke(\n",
        "    {\n",
        "        \"messages\": demo_ephemeral_chat_history.messages,\n",
        "    }\n",
        ")\n",
        "\n",
        "demo_ephemeral_chat_history.add_ai_message(response)\n",
        "\n",
        "input2 = \"What did I just ask you?\"\n",
        "\n",
        "demo_ephemeral_chat_history.add_user_message(input2)\n",
        "\n",
        "chain.invoke(\n",
        "    {\n",
        "        \"messages\": demo_ephemeral_chat_history.messages,\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yueu8QJGRdbe",
        "outputId": "e391b92e-e5e6-41c1-95e1-76ddbc047e8b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='You just asked me to translate the sentence \"I love programming\" from English to French.', response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 74, 'total_tokens': 92}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': 'fp_f93e21ed76', 'finish_reason': 'stop', 'logprobs': None})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Automatic history management"
      ],
      "metadata": {
        "id": "OpVtvnaWSUUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt | chat"
      ],
      "metadata": {
        "id": "a_nyBIgWSKBZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "demo_ephemeral_chat_history_for_chain = ChatMessageHistory()\n",
        "\n",
        "chain_with_message_history = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    lambda session_id: demo_ephemeral_chat_history_for_chain,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"chat_history\",\n",
        ")"
      ],
      "metadata": {
        "id": "UlQkwI3MS5Hl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain_with_message_history.invoke(\n",
        "    {\"input\": \"Translate this sentence from English to French: I love programming.\"},\n",
        "    {\"configurable\": {\"session_id\": \"unused\"}},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5R_NAPQYS6Y8",
        "outputId": "adea438b-e9cf-4e6b-f5c1-11c8f8d0f177"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='The translation of \"I love programming\" to French is \"J\\'adore la programmation.\"', response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 39, 'total_tokens': 59}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': 'fp_9dd82289bf', 'finish_reason': 'stop', 'logprobs': None})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain_with_message_history.invoke(\n",
        "    {\"input\": \"What did I just ask you?\"}, {\"configurable\": {\"session_id\": \"unused\"}}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edcVcKiRTC1s",
        "outputId": "d9f04083-1b06-4fd4-a638-81c090fee331"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='You just asked me to translate the sentence \"I love programming\" from English to French.', response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 74, 'total_tokens': 92}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': 'fp_9dd82289bf', 'finish_reason': 'stop', 'logprobs': None})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modifying chat history"
      ],
      "metadata": {
        "id": "P0mu4-2YTiV8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trimming messages"
      ],
      "metadata": {
        "id": "wC2PLahITpBe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- LLM과 채팅 모델에는 제한된 컨텍스트 창이 있으며, 제한을 직접적으로 맞추지 않더라도 모델이 처리해야 할 주의 분산의 양을 제한\n",
        "\n",
        "- 가장 최근 n개의 메시지만 로드하고 저장"
      ],
      "metadata": {
        "id": "h1PYKR4yXjY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "demo_ephemeral_chat_history = ChatMessageHistory()\n",
        "\n",
        "demo_ephemeral_chat_history.add_user_message(\"Hey there! I'm Nemo.\")\n",
        "demo_ephemeral_chat_history.add_ai_message(\"Hello!\")\n",
        "demo_ephemeral_chat_history.add_user_message(\"How are you today?\")\n",
        "demo_ephemeral_chat_history.add_ai_message(\"Fine thanks!\")\n",
        "\n",
        "demo_ephemeral_chat_history.messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTioMCwzTZmf",
        "outputId": "5b549c76-69d5-4e66-cc42-553b3d5b7755"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content=\"Hey there! I'm Nemo.\"),\n",
              " AIMessage(content='Hello!'),\n",
              " HumanMessage(content='How are you today?'),\n",
              " AIMessage(content='Fine thanks!')]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt | chat\n",
        "\n",
        "chain_with_message_history = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    lambda session_id: demo_ephemeral_chat_history,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"chat_history\",\n",
        ")\n",
        "\n",
        "chain_with_message_history.invoke(\n",
        "    {\"input\": \"What's my name?\"},\n",
        "    {\"configurable\": {\"session_id\": \"unused\"}},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwxCpdyUT5X5",
        "outputId": "a8eb8c63-3783-4106-d068-024401a6e4b0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Your name is Nemo.', response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 66, 'total_tokens': 72}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': 'fp_9dd82289bf', 'finish_reason': 'stop', 'logprobs': None})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 체인에 전달되는 메시지 수를 가장 최근 메시지 2개로만 잘라내기\n",
        "\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "\n",
        "def trim_messages(chain_input):\n",
        "    stored_messages = demo_ephemeral_chat_history.messages\n",
        "    if len(stored_messages) <= 2:\n",
        "        return False\n",
        "\n",
        "    demo_ephemeral_chat_history.clear()\n",
        "\n",
        "    for message in stored_messages[-2:]:\n",
        "        demo_ephemeral_chat_history.add_message(message)\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "chain_with_trimming = (\n",
        "    RunnablePassthrough.assign(messages_trimmed=trim_messages)\n",
        "    | chain_with_message_history\n",
        ")"
      ],
      "metadata": {
        "id": "E6T4ByYYT8eP"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain_with_trimming.invoke(\n",
        "    {\"input\": \"Where does P. Sherman live?\"},\n",
        "    {\"configurable\": {\"session_id\": \"unused\"}},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFkIiHKLX5fQ",
        "outputId": "9c7a59d0-67c1-4fec-ad24-dbf6bc2632d0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"P. Sherman's address is 42 Wallaby Way, Sydney.\", response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 53, 'total_tokens': 67}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': 'fp_f93e21ed76', 'finish_reason': 'stop', 'logprobs': None})"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demo_ephemeral_chat_history.messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYRgsF8kZpeR",
        "outputId": "14f7003e-09bb-4a48-aeb2-6f4d8dada2ed"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content=\"What's my name?\"),\n",
              " AIMessage(content='Your name is Nemo.', response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 66, 'total_tokens': 72}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': 'fp_9dd82289bf', 'finish_reason': 'stop', 'logprobs': None}),\n",
              " HumanMessage(content='Where does P. Sherman live?'),\n",
              " AIMessage(content=\"P. Sherman's address is 42 Wallaby Way, Sydney.\", response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 53, 'total_tokens': 67}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': 'fp_f93e21ed76', 'finish_reason': 'stop', 'logprobs': None})]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary memory"
      ],
      "metadata": {
        "id": "jbhXD-cQaYhQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "demo_ephemeral_chat_history = ChatMessageHistory()\n",
        "\n",
        "demo_ephemeral_chat_history.add_user_message(\"Hey there! I'm Nemo.\")\n",
        "demo_ephemeral_chat_history.add_ai_message(\"Hello!\")\n",
        "demo_ephemeral_chat_history.add_user_message(\"How are you today?\")\n",
        "demo_ephemeral_chat_history.add_ai_message(\"Fine thanks!\")\n",
        "\n",
        "demo_ephemeral_chat_history.messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMHjjOY6ZsKu",
        "outputId": "8eb95cd6-6070-426c-954b-93b0beb53e80"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content=\"Hey there! I'm Nemo.\"),\n",
              " AIMessage(content='Hello!'),\n",
              " HumanMessage(content='How are you today?'),\n",
              " AIMessage(content='Fine thanks!')]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Answer all questions to the best of your ability. The provided chat history includes facts about the user you are speaking with.\",\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        (\"user\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt | chat\n",
        "\n",
        "chain_with_message_history = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    lambda session_id: demo_ephemeral_chat_history,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"chat_history\",\n",
        ")"
      ],
      "metadata": {
        "id": "I5gSxDTJa4fa"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_messages(chain_input):\n",
        "    stored_messages = demo_ephemeral_chat_history.messages\n",
        "    if len(stored_messages) == 0:\n",
        "        return False\n",
        "    summarization_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "            (\n",
        "                \"user\",\n",
        "                \"Distill the above chat messages into a single summary message. Include as many specific details as you can.\",\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "    summarization_chain = summarization_prompt | chat\n",
        "\n",
        "    summary_message = summarization_chain.invoke({\"chat_history\": stored_messages})\n",
        "\n",
        "    demo_ephemeral_chat_history.clear()\n",
        "\n",
        "    demo_ephemeral_chat_history.add_message(summary_message)\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "chain_with_summarization = (\n",
        "    RunnablePassthrough.assign(messages_summarized=summarize_messages)\n",
        "    | chain_with_message_history\n",
        ")"
      ],
      "metadata": {
        "id": "ra4f29lRa8BO"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain_with_summarization.invoke(\n",
        "    {\"input\": \"What did I say my name was?\"},\n",
        "    {\"configurable\": {\"session_id\": \"unused\"}},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghAB33kibEtK",
        "outputId": "a2027d2c-3704-4bcd-f5d7-be33b81f268b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Your name is Nemo.', response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 101, 'total_tokens': 107}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': 'fp_9dd82289bf', 'finish_reason': 'stop', 'logprobs': None})"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demo_ephemeral_chat_history.messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8IZ0VRUbNah",
        "outputId": "7e15d3e3-4439-4778-b335-7e092536f544"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[AIMessage(content='The conversation is between two individuals, Nemo and an unnamed person. Nemo introduces himself and the other person responds with a greeting. The other person then asks how Nemo is doing, to which Nemo responds that he is fine.', response_metadata={'token_usage': {'completion_tokens': 48, 'prompt_tokens': 62, 'total_tokens': 110}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': 'fp_9dd82289bf', 'finish_reason': 'stop', 'logprobs': None}),\n",
              " HumanMessage(content='What did I say my name was?'),\n",
              " AIMessage(content='Your name is Nemo.', response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 101, 'total_tokens': 107}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': 'fp_9dd82289bf', 'finish_reason': 'stop', 'logprobs': None})]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval"
      ],
      "metadata": {
        "id": "AqqVMDvJdVWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --quiet langchain langchain-openai chromadb beautifulsoup4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYrtTRCFdWOI",
        "outputId": "b4c52f8e-19ff-4b1f-d3f8-473d4b4f9cb8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m525.5/525.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.7/105.7 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = 'sk-********************************************************'"
      ],
      "metadata": {
        "id": "yA0T5DHadZ1y"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chat = ChatOpenAI(model=\"gpt-3.5-turbo-1106\", temperature=0.2)"
      ],
      "metadata": {
        "id": "HiyPeL7_dn30"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retriever 생성"
      ],
      "metadata": {
        "id": "9_qQ9UEHdvZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 문서 로더를 사용하여 문서에서 텍스트를 가져오기\n",
        "\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "loader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")\n",
        "data = loader.load()"
      ],
      "metadata": {
        "id": "NCysAOLhduTy"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 다음으로 이를 LLM의 컨텍스트 창이 처리할 수 있는 더 작은 청크로 분할\n",
        "\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
        "all_splits = text_splitter.split_documents(data)"
      ],
      "metadata": {
        "id": "QK8GqGA0d1-K"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 해당 청크를 벡터 데이터베이스에 저장\n",
        "\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())"
      ],
      "metadata": {
        "id": "cvemFPyreBHw"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# k: 검색할 청크 수입니다.\n",
        "retriever = vectorstore.as_retriever(k=4)\n",
        "\n",
        "docs = retriever.invoke(\"Can LangSmith help test my LLM applications?\")\n",
        "\n",
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abOFs1uDeKiH",
        "outputId": "1c48a827-089a-42ab-ca3b-5e48314c9d46"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='go-to open source framework for building with LLMs.LangSmith is developed by LangChain, the company behind the open source LangChain framework.Quick Start‚ÄãTracing: Get started with the tracing quick start.Evaluation: Get started with the evaluation quick start.Next Steps‚ÄãCheck out the following sections to learn more about LangSmith:User Guide: Learn about the workflows LangSmith supports at each stage of the LLM application lifecycle.Setup: Learn how to create an account, obtain an API', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n",
              " Document(page_content='LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n",
              " Document(page_content='Skip to main content\\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith DocsLangChain Python DocsLangChain JS/TS DocsLangSmith API DocsSearchGo to AppLangSmithUser GuideSetupPricing (Coming Soon)Self-HostingTracingEvaluationMonitoringPrompt HubProxyCookbookLangSmithOn this pageLangSmithIntroduction‚ÄãLangSmith is a platform for building production-grade LLM applications.It lets you debug, test, evaluate, and monitor chains and intelligent agents built on any LLM framework and seamlessly integrates with LangChain, the', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n",
              " Document(page_content='key, and configure your environment.Pricing: Learn about the pricing model for LangSmith.Self-Hosting: Learn about self-hosting options for LangSmith.Proxy: Learn about the proxy capabilities of LangSmith.Tracing: Learn about the tracing capabilities of LangSmith.Evaluation: Learn about the evaluation capabilities of LangSmith.Prompt Hub Learn about the Prompt Hub, a prompt management tool built into LangSmith.Additional Resources‚ÄãLangSmith Cookbook: A collection of tutorials and end-to-end', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'})]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Document chains"
      ],
      "metadata": {
        "id": "Rgag2WP2ecWz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "# Prompt 생성\n",
        "SYSTEM_TEMPLATE = \"\"\"\n",
        "Answer the user's questions based on the below context.\n",
        "If the context doesn't contain any relevant information to the question, don't make something up and just say \"I don't know\":\n",
        "\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\"\"\"\n",
        "\n",
        "question_answering_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            SYSTEM_TEMPLATE,\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Document 체인 생성\n",
        "document_chain = create_stuff_documents_chain(chat, question_answering_prompt)"
      ],
      "metadata": {
        "id": "mMeZPobyeO1N"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "document_chain.invoke(\n",
        "    {\n",
        "        \"context\": docs,\n",
        "        \"messages\": [\n",
        "            HumanMessage(content=\"Can LangSmith help test my LLM applications?\")\n",
        "        ],\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-1pKhly4ei60",
        "outputId": "b517cd4c-e5b1-4399-eaec-b5314378bc1c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Yes, LangSmith can help test your LLM applications. It provides evaluation capabilities for testing and evaluating chains and intelligent agents built on any LLM framework.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 문서 없이 실행\n",
        "\n",
        "document_chain.invoke(\n",
        "    {\n",
        "        \"context\": [],\n",
        "        \"messages\": [\n",
        "            HumanMessage(content=\"Can LangSmith help test my LLM applications?\")\n",
        "        ],\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "kboiWJcUfB3w",
        "outputId": "a66a9e63-f07d-4412-8fa4-db4920130176"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I don't know.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval chains"
      ],
      "metadata": {
        "id": "vUazSJiyfe-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 입력 메시지 목록이 주어지면 목록에서 마지막 메시지의 내용을 추출하여 일부 문서를 가져오도록 리트리버에 전달\n",
        "# 해당 문서를 컨텍스트로 문서 체인에 전달하여 최종 응답을 생성\n",
        "\n",
        "from typing import Dict\n",
        "\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "\n",
        "def parse_retriever_input(params: Dict):\n",
        "    return params[\"messages\"][-1].content\n",
        "\n",
        "\n",
        "retrieval_chain = RunnablePassthrough.assign(\n",
        "    context=parse_retriever_input | retriever,\n",
        ").assign(\n",
        "    answer=document_chain,\n",
        ")"
      ],
      "metadata": {
        "id": "q_RhkIgifb3j"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieval_chain.invoke(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            HumanMessage(content=\"Can LangSmith help test my LLM applications?\")\n",
        "        ],\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XXUKLoZflr3",
        "outputId": "135eefa0-ec07-4683-a16e-8adfe102c245"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='Can LangSmith help test my LLM applications?')],\n",
              " 'context': [Document(page_content='go-to open source framework for building with LLMs.LangSmith is developed by LangChain, the company behind the open source LangChain framework.Quick Start‚ÄãTracing: Get started with the tracing quick start.Evaluation: Get started with the evaluation quick start.Next Steps‚ÄãCheck out the following sections to learn more about LangSmith:User Guide: Learn about the workflows LangSmith supports at each stage of the LLM application lifecycle.Setup: Learn how to create an account, obtain an API', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n",
              "  Document(page_content='LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n",
              "  Document(page_content='Skip to main content\\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith DocsLangChain Python DocsLangChain JS/TS DocsLangSmith API DocsSearchGo to AppLangSmithUser GuideSetupPricing (Coming Soon)Self-HostingTracingEvaluationMonitoringPrompt HubProxyCookbookLangSmithOn this pageLangSmithIntroduction‚ÄãLangSmith is a platform for building production-grade LLM applications.It lets you debug, test, evaluate, and monitor chains and intelligent agents built on any LLM framework and seamlessly integrates with LangChain, the', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n",
              "  Document(page_content='key, and configure your environment.Pricing: Learn about the pricing model for LangSmith.Self-Hosting: Learn about self-hosting options for LangSmith.Proxy: Learn about the proxy capabilities of LangSmith.Tracing: Learn about the tracing capabilities of LangSmith.Evaluation: Learn about the evaluation capabilities of LangSmith.Prompt Hub Learn about the Prompt Hub, a prompt management tool built into LangSmith.Additional Resources‚ÄãLangSmith Cookbook: A collection of tutorials and end-to-end', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'})],\n",
              " 'answer': 'Yes, LangSmith can help test your LLM applications. It provides capabilities for evaluation, tracing, and monitoring chains and intelligent agents built on any LLM framework.'}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query transformation"
      ],
      "metadata": {
        "id": "5nb5GtRpgnvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.invoke(\"Tell me more!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BKt4mUtgkYT",
        "outputId": "1b47e9f0-77dd-4560-c0ba-9ed15ab54fca"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n",
              " Document(page_content='GuideIntroductionQuick StartNext StepsAdditional ResourcesCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogCopyright ¬© 2024 LangChain, Inc.', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n",
              " Document(page_content='key, and configure your environment.Pricing: Learn about the pricing model for LangSmith.Self-Hosting: Learn about self-hosting options for LangSmith.Proxy: Learn about the proxy capabilities of LangSmith.Tracing: Learn about the tracing capabilities of LangSmith.Evaluation: Learn about the evaluation capabilities of LangSmith.Prompt Hub Learn about the Prompt Hub, a prompt management tool built into LangSmith.Additional Resources‚ÄãLangSmith Cookbook: A collection of tutorials and end-to-end', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n",
              " Document(page_content='Skip to main content\\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith DocsLangChain Python DocsLangChain JS/TS DocsLangSmith API DocsSearchGo to AppLangSmithUser GuideSetupPricing (Coming Soon)Self-HostingTracingEvaluationMonitoringPrompt HubProxyCookbookLangSmithOn this pageLangSmithIntroduction‚ÄãLangSmith is a platform for building production-grade LLM applications.It lets you debug, test, evaluate, and monitor chains and intelligent agents built on any LLM framework and seamlessly integrates with LangChain, the', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'})]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "\n",
        "query_transform_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "        (\n",
        "            \"user\",\n",
        "            \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation. Only respond with the query, nothing else.\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "query_transformation_chain = query_transform_prompt | chat\n",
        "\n",
        "query_transformation_chain.invoke(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            HumanMessage(content=\"Can LangSmith help test my LLM applications?\"),\n",
        "            AIMessage(\n",
        "                content=\"Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.\"\n",
        "            ),\n",
        "            HumanMessage(content=\"Tell me more!\"),\n",
        "        ],\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSyX_t8JioDf",
        "outputId": "c9e633bf-7fc8-47d9-c08a-ac74b0700421"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='\"LangSmith LLM application testing and evaluation\"', response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 145, 'total_tokens': 155}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': 'fp_9dd82289bf', 'finish_reason': 'stop', 'logprobs': None})"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableBranch\n",
        "\n",
        "query_transforming_retriever_chain = RunnableBranch(\n",
        "    (\n",
        "        lambda x: len(x.get(\"messages\", [])) == 1,\n",
        "        # If only one message, then we just pass that message's content to retriever\n",
        "        (lambda x: x[\"messages\"][-1].content) | retriever,\n",
        "    ),\n",
        "    # If messages, then we pass inputs to LLM chain to transform the query, then pass to retriever\n",
        "    query_transform_prompt | chat | StrOutputParser() | retriever,\n",
        ").with_config(run_name=\"chat_retriever_chain\")"
      ],
      "metadata": {
        "id": "AWc216GejZL_"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_TEMPLATE = \"\"\"\n",
        "Answer the user's questions based on the below context.\n",
        "If the context doesn't contain any relevant information to the question, don't make something up and just say \"I don't know\":\n",
        "\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\"\"\"\n",
        "\n",
        "question_answering_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            SYSTEM_TEMPLATE,\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "document_chain = create_stuff_documents_chain(chat, question_answering_prompt)\n",
        "\n",
        "conversational_retrieval_chain = RunnablePassthrough.assign(\n",
        "    context=query_transforming_retriever_chain,\n",
        ").assign(\n",
        "    answer=document_chain,\n",
        ")"
      ],
      "metadata": {
        "id": "lbq7SisOjilw"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversational_retrieval_chain.invoke(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            HumanMessage(content=\"Can LangSmith help test my LLM applications?\"),\n",
        "        ]\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1CEGaW9j_5d",
        "outputId": "6738ccd5-2af8-451f-96c9-b38ad32bd664"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='Can LangSmith help test my LLM applications?')],\n",
              " 'context': [Document(page_content='go-to open source framework for building with LLMs.LangSmith is developed by LangChain, the company behind the open source LangChain framework.Quick Start‚ÄãTracing: Get started with the tracing quick start.Evaluation: Get started with the evaluation quick start.Next Steps‚ÄãCheck out the following sections to learn more about LangSmith:User Guide: Learn about the workflows LangSmith supports at each stage of the LLM application lifecycle.Setup: Learn how to create an account, obtain an API', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n",
              "  Document(page_content='LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n",
              "  Document(page_content='Skip to main content\\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith DocsLangChain Python DocsLangChain JS/TS DocsLangSmith API DocsSearchGo to AppLangSmithUser GuideSetupPricing (Coming Soon)Self-HostingTracingEvaluationMonitoringPrompt HubProxyCookbookLangSmithOn this pageLangSmithIntroduction‚ÄãLangSmith is a platform for building production-grade LLM applications.It lets you debug, test, evaluate, and monitor chains and intelligent agents built on any LLM framework and seamlessly integrates with LangChain, the', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n",
              "  Document(page_content='key, and configure your environment.Pricing: Learn about the pricing model for LangSmith.Self-Hosting: Learn about self-hosting options for LangSmith.Proxy: Learn about the proxy capabilities of LangSmith.Tracing: Learn about the tracing capabilities of LangSmith.Evaluation: Learn about the evaluation capabilities of LangSmith.Prompt Hub Learn about the Prompt Hub, a prompt management tool built into LangSmith.Additional Resources‚ÄãLangSmith Cookbook: A collection of tutorials and end-to-end', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'})],\n",
              " 'answer': 'Yes, LangSmith can help you test your LLM applications. It provides capabilities for debugging, testing, evaluating, and monitoring chains and intelligent agents built on any LLM framework. You can get started with the evaluation quick start to learn more about how LangSmith supports testing LLM applications.'}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversational_retrieval_chain.invoke(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            HumanMessage(content=\"Can LangSmith help test my LLM applications?\"),\n",
        "            AIMessage(\n",
        "                content=\"Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.\"\n",
        "            ),\n",
        "            HumanMessage(content=\"Tell me more!\"),\n",
        "        ],\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOiAo4KZkC6S",
        "outputId": "85fae410-4ba6-4218-c5fd-bddb48fdb538"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='Can LangSmith help test my LLM applications?'),\n",
              "  AIMessage(content='Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.'),\n",
              "  HumanMessage(content='Tell me more!')],\n",
              " 'context': [Document(page_content='LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n",
              "  Document(page_content='go-to open source framework for building with LLMs.LangSmith is developed by LangChain, the company behind the open source LangChain framework.Quick Start‚ÄãTracing: Get started with the tracing quick start.Evaluation: Get started with the evaluation quick start.Next Steps‚ÄãCheck out the following sections to learn more about LangSmith:User Guide: Learn about the workflows LangSmith supports at each stage of the LLM application lifecycle.Setup: Learn how to create an account, obtain an API', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n",
              "  Document(page_content='Skip to main content\\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith DocsLangChain Python DocsLangChain JS/TS DocsLangSmith API DocsSearchGo to AppLangSmithUser GuideSetupPricing (Coming Soon)Self-HostingTracingEvaluationMonitoringPrompt HubProxyCookbookLangSmithOn this pageLangSmithIntroduction‚ÄãLangSmith is a platform for building production-grade LLM applications.It lets you debug, test, evaluate, and monitor chains and intelligent agents built on any LLM framework and seamlessly integrates with LangChain, the', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n",
              "  Document(page_content='key, and configure your environment.Pricing: Learn about the pricing model for LangSmith.Self-Hosting: Learn about self-hosting options for LangSmith.Proxy: Learn about the proxy capabilities of LangSmith.Tracing: Learn about the tracing capabilities of LangSmith.Evaluation: Learn about the evaluation capabilities of LangSmith.Prompt Hub Learn about the Prompt Hub, a prompt management tool built into LangSmith.Additional Resources‚ÄãLangSmith Cookbook: A collection of tutorials and end-to-end', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'})],\n",
              " 'answer': \"LangSmith is a platform designed specifically for building production-grade LLM applications. It provides capabilities for debugging, testing, evaluating, and monitoring chains and intelligent agents built on any LLM framework. Additionally, LangSmith seamlessly integrates with LangChain, the key, and allows you to configure your environment. It also offers features such as tracing, evaluation, and a Prompt Hub, which is a prompt management tool built into LangSmith. If you're interested in learning more, you can explore the User Guide to understand the workflows supported at each stage of the LLM application lifecycle, as well as the Setup section to learn how to create an account and obtain an API key.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Streaming"
      ],
      "metadata": {
        "id": "Y1x-NeaskSFW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stream = conversational_retrieval_chain.stream(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            HumanMessage(content=\"Can LangSmith help test my LLM applications?\"),\n",
        "            AIMessage(\n",
        "                content=\"Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.\"\n",
        "            ),\n",
        "            HumanMessage(content=\"Tell me more!\"),\n",
        "        ],\n",
        "    }\n",
        ")\n",
        "\n",
        "for chunk in stream:\n",
        "    print(chunk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_YVwQm6kFMY",
        "outputId": "4484184e-e033-4952-be51-088b88f718b4"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'messages': [HumanMessage(content='Can LangSmith help test my LLM applications?'), AIMessage(content='Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.'), HumanMessage(content='Tell me more!')]}\n",
            "{'context': [Document(page_content='LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}), Document(page_content='go-to open source framework for building with LLMs.LangSmith is developed by LangChain, the company behind the open source LangChain framework.Quick Start‚ÄãTracing: Get started with the tracing quick start.Evaluation: Get started with the evaluation quick start.Next Steps‚ÄãCheck out the following sections to learn more about LangSmith:User Guide: Learn about the workflows LangSmith supports at each stage of the LLM application lifecycle.Setup: Learn how to create an account, obtain an API', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}), Document(page_content='Skip to main content\\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith DocsLangChain Python DocsLangChain JS/TS DocsLangSmith API DocsSearchGo to AppLangSmithUser GuideSetupPricing (Coming Soon)Self-HostingTracingEvaluationMonitoringPrompt HubProxyCookbookLangSmithOn this pageLangSmithIntroduction‚ÄãLangSmith is a platform for building production-grade LLM applications.It lets you debug, test, evaluate, and monitor chains and intelligent agents built on any LLM framework and seamlessly integrates with LangChain, the', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}), Document(page_content='key, and configure your environment.Pricing: Learn about the pricing model for LangSmith.Self-Hosting: Learn about self-hosting options for LangSmith.Proxy: Learn about the proxy capabilities of LangSmith.Tracing: Learn about the tracing capabilities of LangSmith.Evaluation: Learn about the evaluation capabilities of LangSmith.Prompt Hub Learn about the Prompt Hub, a prompt management tool built into LangSmith.Additional Resources‚ÄãLangSmith Cookbook: A collection of tutorials and end-to-end', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'})]}\n",
            "{'answer': ''}\n",
            "{'answer': 'Lang'}\n",
            "{'answer': 'Smith'}\n",
            "{'answer': ' is'}\n",
            "{'answer': ' a'}\n",
            "{'answer': ' platform'}\n",
            "{'answer': ' designed'}\n",
            "{'answer': ' for'}\n",
            "{'answer': ' building'}\n",
            "{'answer': ' production'}\n",
            "{'answer': '-grade'}\n",
            "{'answer': ' L'}\n",
            "{'answer': 'LM'}\n",
            "{'answer': ' ('}\n",
            "{'answer': 'Language'}\n",
            "{'answer': ' Model'}\n",
            "{'answer': ')'}\n",
            "{'answer': ' applications'}\n",
            "{'answer': '.'}\n",
            "{'answer': ' It'}\n",
            "{'answer': ' offers'}\n",
            "{'answer': ' capabilities'}\n",
            "{'answer': ' to'}\n",
            "{'answer': ' debug'}\n",
            "{'answer': ','}\n",
            "{'answer': ' test'}\n",
            "{'answer': ','}\n",
            "{'answer': ' evaluate'}\n",
            "{'answer': ','}\n",
            "{'answer': ' and'}\n",
            "{'answer': ' monitor'}\n",
            "{'answer': ' chains'}\n",
            "{'answer': ' and'}\n",
            "{'answer': ' intelligent'}\n",
            "{'answer': ' agents'}\n",
            "{'answer': ' built'}\n",
            "{'answer': ' on'}\n",
            "{'answer': ' any'}\n",
            "{'answer': ' L'}\n",
            "{'answer': 'LM'}\n",
            "{'answer': ' framework'}\n",
            "{'answer': '.'}\n",
            "{'answer': ' It'}\n",
            "{'answer': ' seamlessly'}\n",
            "{'answer': ' integrates'}\n",
            "{'answer': ' with'}\n",
            "{'answer': ' Lang'}\n",
            "{'answer': 'Chain'}\n",
            "{'answer': ','}\n",
            "{'answer': ' the'}\n",
            "{'answer': ' open'}\n",
            "{'answer': ' source'}\n",
            "{'answer': ' framework'}\n",
            "{'answer': ' developed'}\n",
            "{'answer': ' by'}\n",
            "{'answer': ' Lang'}\n",
            "{'answer': 'Chain'}\n",
            "{'answer': '.'}\n",
            "{'answer': ' \\n\\n'}\n",
            "{'answer': 'In'}\n",
            "{'answer': ' terms'}\n",
            "{'answer': ' of'}\n",
            "{'answer': ' testing'}\n",
            "{'answer': ','}\n",
            "{'answer': ' Lang'}\n",
            "{'answer': 'Smith'}\n",
            "{'answer': ' provides'}\n",
            "{'answer': ' tracing'}\n",
            "{'answer': ' and'}\n",
            "{'answer': ' evaluation'}\n",
            "{'answer': ' capabilities'}\n",
            "{'answer': '.'}\n",
            "{'answer': ' Tr'}\n",
            "{'answer': 'acing'}\n",
            "{'answer': ' allows'}\n",
            "{'answer': ' you'}\n",
            "{'answer': ' to'}\n",
            "{'answer': ' trace'}\n",
            "{'answer': ' the'}\n",
            "{'answer': ' execution'}\n",
            "{'answer': ' of'}\n",
            "{'answer': ' your'}\n",
            "{'answer': ' L'}\n",
            "{'answer': 'LM'}\n",
            "{'answer': ' applications'}\n",
            "{'answer': ','}\n",
            "{'answer': ' while'}\n",
            "{'answer': ' evaluation'}\n",
            "{'answer': ' enables'}\n",
            "{'answer': ' you'}\n",
            "{'answer': ' to'}\n",
            "{'answer': ' test'}\n",
            "{'answer': ' and'}\n",
            "{'answer': ' evaluate'}\n",
            "{'answer': ' the'}\n",
            "{'answer': ' performance'}\n",
            "{'answer': ' of'}\n",
            "{'answer': ' your'}\n",
            "{'answer': ' language'}\n",
            "{'answer': ' models'}\n",
            "{'answer': '.'}\n",
            "{'answer': ' Additionally'}\n",
            "{'answer': ','}\n",
            "{'answer': ' Lang'}\n",
            "{'answer': 'Smith'}\n",
            "{'answer': ' offers'}\n",
            "{'answer': ' a'}\n",
            "{'answer': ' Prompt'}\n",
            "{'answer': ' Hub'}\n",
            "{'answer': ','}\n",
            "{'answer': ' which'}\n",
            "{'answer': ' is'}\n",
            "{'answer': ' a'}\n",
            "{'answer': ' prompt'}\n",
            "{'answer': ' management'}\n",
            "{'answer': ' tool'}\n",
            "{'answer': ' built'}\n",
            "{'answer': ' into'}\n",
            "{'answer': ' the'}\n",
            "{'answer': ' platform'}\n",
            "{'answer': '.\\n\\n'}\n",
            "{'answer': 'If'}\n",
            "{'answer': ' you'}\n",
            "{'answer': \"'re\"}\n",
            "{'answer': ' interested'}\n",
            "{'answer': ' in'}\n",
            "{'answer': ' learning'}\n",
            "{'answer': ' more'}\n",
            "{'answer': ','}\n",
            "{'answer': ' you'}\n",
            "{'answer': ' can'}\n",
            "{'answer': ' explore'}\n",
            "{'answer': ' the'}\n",
            "{'answer': ' User'}\n",
            "{'answer': ' Guide'}\n",
            "{'answer': ' to'}\n",
            "{'answer': ' understand'}\n",
            "{'answer': ' the'}\n",
            "{'answer': ' workflows'}\n",
            "{'answer': ' supported'}\n",
            "{'answer': ' by'}\n",
            "{'answer': ' Lang'}\n",
            "{'answer': 'Smith'}\n",
            "{'answer': ' at'}\n",
            "{'answer': ' each'}\n",
            "{'answer': ' stage'}\n",
            "{'answer': ' of'}\n",
            "{'answer': ' the'}\n",
            "{'answer': ' L'}\n",
            "{'answer': 'LM'}\n",
            "{'answer': ' application'}\n",
            "{'answer': ' lifecycle'}\n",
            "{'answer': ','}\n",
            "{'answer': ' as'}\n",
            "{'answer': ' well'}\n",
            "{'answer': ' as'}\n",
            "{'answer': ' the'}\n",
            "{'answer': ' Setup'}\n",
            "{'answer': ' section'}\n",
            "{'answer': ' to'}\n",
            "{'answer': ' learn'}\n",
            "{'answer': ' how'}\n",
            "{'answer': ' to'}\n",
            "{'answer': ' create'}\n",
            "{'answer': ' an'}\n",
            "{'answer': ' account'}\n",
            "{'answer': ' and'}\n",
            "{'answer': ' obtain'}\n",
            "{'answer': ' an'}\n",
            "{'answer': ' API'}\n",
            "{'answer': ' key'}\n",
            "{'answer': '.'}\n",
            "{'answer': ''}\n"
          ]
        }
      ]
    }
  ]
}