{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9fa23b62",
      "metadata": {
        "id": "9fa23b62"
      },
      "source": [
        "# Splitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "id": "354b1e04",
      "metadata": {
        "id": "354b1e04"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain langchain-openai langchain_community langchain_experimental langchain-text-splitters tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8U_lhAypNH9G",
      "metadata": {
        "id": "8U_lhAypNH9G"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = 'sk-***********************************************'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qGP4LnO6kSHN",
      "metadata": {
        "id": "qGP4LnO6kSHN"
      },
      "source": [
        "## CharacterTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "id": "X4rH1amcQCEw",
      "metadata": {
        "id": "X4rH1amcQCEw"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/appendix-keywords.txt\") as f:\n",
        "    file = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "id": "T40PysHVQfaH",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T40PysHVQfaH",
        "outputId": "1b3c65a6-a528-489c-dd33-6a66803144c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Semantic Search\n",
            "\n",
            "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
            "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
            "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
            "\n",
            "Embedding\n",
            "\n",
            "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
            "예시: \"사과\"라는 단\n"
          ]
        }
      ],
      "source": [
        "print(file[:300])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "id": "0F6YtNxXQmOf",
      "metadata": {
        "id": "0F6YtNxXQmOf"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "\n",
        "text_splitter = CharacterTextSplitter(\n",
        "     # Specify the delimiter to use when splitting text. The default is \"\\n\\n\".\n",
        "     # separator=\" \",\n",
        "     # Specifies the maximum size of split text chunks.\n",
        "     chunk_size=250,\n",
        "     # Specifies the number of overlapping characters between split text chunks.\n",
        "     chunk_overlap=50,\n",
        "     # Specify a function to calculate the length of text.\n",
        "     length_function=len,\n",
        "     # Specifies whether the separator is a regular expression.\n",
        "     is_separator_regex=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "id": "t_ya0XeEQ6Fv",
      "metadata": {
        "id": "t_ya0XeEQ6Fv"
      },
      "outputs": [],
      "source": [
        "texts = text_splitter.create_documents([file])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "id": "YGXxTX_uQ8s3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGXxTX_uQ8s3",
        "outputId": "1e82a690-0513-412e-d3b8-a915600fbcfb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "execution_count": 143,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "id": "qjdVmx_5Q_BG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjdVmx_5Q_BG",
        "outputId": "58fd666e-2733-4cd9-c34b-5679d3fcac41"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(page_content='Semantic Search\\n\\n정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\\n예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\\n연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\\n\\nEmbedding')"
            ]
          },
          "execution_count": 144,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "texts[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tXY97yRnRMdH",
      "metadata": {
        "id": "tXY97yRnRMdH"
      },
      "source": [
        "## RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b08607a",
      "metadata": {},
      "source": [
        "`RecursiveCharacterTextSplitter`는 일반적인 텍스트에 권장되는 방식으로 문자 목록을 매개변수로 받아 동작합니다.\n",
        "\n",
        "기본 문자 목록은 `[\"\\n\\n\", \"\\n\", \" \", \"\"]`입니다.\n",
        "\n",
        "청크가 충분히 작아질 때까지 주어진 문자 목록의 순서대로 텍스트를 분할하려고 시도합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "id": "MaSjdJ9TRQpW",
      "metadata": {
        "id": "MaSjdJ9TRQpW"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "id": "WT4EWlxGRUzX",
      "metadata": {
        "id": "WT4EWlxGRUzX"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "     # Set the chunk size to be very small. This setting is for example purposes only.\n",
        "     chunk_size=250,\n",
        "     # Set the number of overlapping characters between chunks.\n",
        "     chunk_overlap=50,\n",
        "     # Specify a function to calculate the string length.\n",
        "     length_function=len,\n",
        "     # Set whether to use regular expressions as delimiters.\n",
        "     is_separator_regex=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "id": "xmVucFhdRdQQ",
      "metadata": {
        "id": "xmVucFhdRdQQ"
      },
      "outputs": [],
      "source": [
        "texts = text_splitter.create_documents([file])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "id": "maC1tL2QReof",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maC1tL2QReof",
        "outputId": "c818b5a6-67f3-468c-dc01-6a533946fbed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "execution_count": 148,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "id": "RWwHXxyvRi1n",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWwHXxyvRi1n",
        "outputId": "4aae5851-d707-4bdf-fbf8-36a7cca5a703"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(page_content='Semantic Search\\n\\n정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\\n예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\\n연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\\n\\nEmbedding')"
            ]
          },
          "execution_count": 149,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "texts[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "om4Ia_HXSGTu",
      "metadata": {
        "id": "om4Ia_HXSGTu"
      },
      "source": [
        "## TokenTextSplitter"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Re98qjVWShv2",
      "metadata": {
        "id": "Re98qjVWShv2"
      },
      "source": [
        "### Tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "id": "ddnf3m3ySj52",
      "metadata": {
        "id": "ddnf3m3ySj52"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "\n",
        "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
        "     # Set chunk size to 300.\n",
        "     chunk_size=500,\n",
        "     # Set so that there is no overlap between chunks.\n",
        "     chunk_overlap=0,\n",
        ")\n",
        "# split the file text into chunks.\n",
        "texts = text_splitter.split_text(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "id": "6OKJC5a9S6au",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OKJC5a9S6au",
        "outputId": "45b8a1ce-37c0-4fb3-c3ff-bd418b478d47"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "execution_count": 155,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "id": "xAXCF--nSuwm",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "xAXCF--nSuwm",
        "outputId": "da2992b7-2fab-4f30-9db5-223c9489c0c4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Semantic Search\\n\\n정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\\n예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\\n연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\\n\\nEmbedding'"
            ]
          },
          "execution_count": 154,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "texts[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vLhbIX51TEQ1",
      "metadata": {
        "id": "vLhbIX51TEQ1"
      },
      "source": [
        "### TokenTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "id": "OL_XGS67TFbW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OL_XGS67TFbW",
        "outputId": "ce20ad0d-e46b-4ba0-ceb7-61ae9696476a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Semantic Search\n",
            "\n",
            "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
            "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
            "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
            "\n",
            "Embedding\n",
            "\n",
            "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 �\n"
          ]
        }
      ],
      "source": [
        "from langchain_text_splitters import TokenTextSplitter\n",
        "\n",
        "text_splitter = TokenTextSplitter(\n",
        "     chunk_size=500, # Set chunk size to 10.\n",
        "     chunk_overlap=0, # Set overlap between chunks to 0.\n",
        ")\n",
        "\n",
        "# state_of_the_union Split the text into chunks.\n",
        "texts = text_splitter.split_text(file)\n",
        "print(texts[0]) # Print the first chunk of segmented text"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
