{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RunnableParallel"
      ],
      "metadata": {
        "id": "259M-8OXmQD7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "4Vahr4cWmPN6"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --quiet  langchain-core langchain-community langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"]      = \"sk-*******************************************\""
      ],
      "metadata": {
        "id": "86kvSCFwnI2s"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "\n",
        "vectorstore = FAISS.from_texts(\n",
        "    [\"harrison worked at kensho\"], embedding=OpenAIEmbeddings()\n",
        ")\n",
        "retriever = vectorstore.as_retriever()\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "model = ChatOpenAI()\n",
        "\n",
        "retrieval_chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "retrieval_chain.invoke(\"where did harrison work?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Yd-89JGxDJcH",
        "outputId": "8c26ec0a-8de4-411e-e861-6f7f7e5f9aeb"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Harrison worked at Kensho.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableParallel\n",
        "\n",
        "retrieval_chain2 = (\n",
        "    RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "retrieval_chain2.invoke(\"where did harrison work?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZuiRSmvkDdaf",
        "outputId": "f03d42fc-5b76-4517-f177-0055f4041527"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Harrison worked at Kensho.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableParallel\n",
        "\n",
        "retrieval_chain3 = (\n",
        "    RunnableParallel(context=retriever, question=RunnablePassthrough())\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "retrieval_chain3.invoke(\"where did harrison work?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RAvP0j2uERZz",
        "outputId": "06881b99-99bd-40f7-fbfd-fa34efe43454"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Harrison worked at Kensho.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parallelize steps"
      ],
      "metadata": {
        "id": "rKd-u-pLEfQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableParallel\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI()\n",
        "joke_chain = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\") | model\n",
        "poem_chain = (\n",
        "    ChatPromptTemplate.from_template(\"write a 2-line poem about {topic}\") | model\n",
        ")\n",
        "\n",
        "map_chain = RunnableParallel(joke=joke_chain, poem=poem_chain)\n",
        "\n",
        "map_chain.invoke({\"topic\": \"bear\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JtNM-0wEghp",
        "outputId": "d86bd9cc-fb31-4649-a307-288b043eecef"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'joke': AIMessage(content='Why did the bear bring a flashlight to the party? \\n\\nBecause he heard it was going to be a \"beary\" bright affair!', response_metadata={'finish_reason': 'stop', 'logprobs': None}),\n",
              " 'poem': AIMessage(content='In the quiet forest, the bear roams free,\\nMajestic and strong, a sight to see.', response_metadata={'finish_reason': 'stop', 'logprobs': None})}"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit\n",
        "\n",
        "joke_chain.invoke({\"topic\": \"bear\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8YpV1IKFLkF",
        "outputId": "f54bc882-33fc-4ce8-8223-959a37bd27ea"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "896 ms ± 180 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit\n",
        "\n",
        "poem_chain.invoke({\"topic\": \"bear\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAWggu05FOoK",
        "outputId": "31bb3c53-bcb2-4390-8bc2-d6d90b07b663"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "727 ms ± 146 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit\n",
        "\n",
        "map_chain.invoke({\"topic\": \"bear\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qCIK_IHFRzM",
        "outputId": "d1902508-952d-419b-e6bf-a006c40e6eed"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "889 ms ± 201 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ]
    }
  ]
}
