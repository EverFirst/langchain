{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "259M-8OXmQD7"
      },
      "source": [
        "# RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1. 필수 라이브러리 설치\n",
        "faiss-cpu : Faiss Vector DB \n",
        "\n",
        "tiktoken : 토큰 수 계산"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Vahr4cWmPN6",
        "outputId": "ad7d5370-d6c0-4bc9-f34f-dbb7bba548a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --quiet  langchain langchain-openai faiss-cpu tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2. OpenAI API Key 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "86kvSCFwnI2s"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"]      = \"sk-*******************************************************\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KU6Zg_A0CYoa"
      },
      "source": [
        "## Retrieval Chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vector Databse에 저장되어 있는 데이터를 Retrieval (검색) 하여 가져오는 체인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "G0qTpQ-JB3GQ",
        "outputId": "edad5adc-7803-44f3-95b2-f681f63585c0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Harrison worked at Kensho.'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "\n",
        "#Faiss Vector Database에 \"harrison worked at kensho\" 라는 문장을 텍스트로 저장함\n",
        "vectorstore = FAISS.from_texts(\n",
        "    [\"harrison worked at kensho\"], embedding=OpenAIEmbeddings()\n",
        ")\n",
        "\n",
        "#Vector DB를 검색기에 연결\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "#Prompt Template 생성\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "#모델 설정\n",
        "model = ChatOpenAI()\n",
        "\n",
        "# Prompt에 들어가는 변수 지정\n",
        "# 검색대상 : \"context\"          /  질문 : Input Value\n",
        "# String 데이터 타입으로 Output 반환\n",
        "chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "chain.invoke(\"where did harrison work?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "iDYAZfLBCODB",
        "outputId": "26514beb-9d3a-4135-adbc-707d040af462"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'해리슨은 켄쇼에서 일했습니다.'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Prompt Template 설정\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer in the following language: {language}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "#context는 직접적인 질문 또는 vector DB 검색기가 될 수 있음  (질문에 답변이 있을 수 있음)\n",
        "chain = (\n",
        "    {\n",
        "        \"context\": itemgetter(\"question\") | retriever,\n",
        "        \"question\": itemgetter(\"question\"),\n",
        "        \"language\": itemgetter(\"language\"),\n",
        "    }\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "chain.invoke({\"question\": \"where did harrison work\", \"language\": \"korean\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "on3jypqeCblu"
      },
      "source": [
        "## Conversational Retrieval Chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "대화 형식을 통해 검색기를 사용하는 Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0kdKJnXzCdED"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage, get_buffer_string\n",
        "from langchain_core.prompts import format_document\n",
        "from langchain_core.runnables import RunnableParallel\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "\n",
        "#Prompt Template 생성\n",
        "#질문 Template 생성\n",
        "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
        "\n",
        "Chat History:\n",
        "{chat_history}\n",
        "Follow Up Input: {question}\n",
        "Standalone question:\"\"\"\n",
        "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
        "\n",
        "#답변 Template 생성\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "ANSWER_PROMPT = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "#문서 Template 생성\n",
        "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n",
        "\n",
        "#여러 문서를 결합하는 함수 생성\n",
        "def _combine_documents(\n",
        "    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"\n",
        "):\n",
        "    #여러 docs의 내용을 기반으로, 문서 별로 문서 Prompt 실행\n",
        "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
        "\n",
        "    #여러 문서를 '\\n\\n' 구분 기호로 구분하여 하나의 문서로 결합\n",
        "    return document_separator.join(doc_strings)\n",
        "\n",
        "#Input 체인 생성\n",
        "#assign을 활용하여 변수 미리 지정\n",
        "#Buffer를 활용하여 Chat History 생성\n",
        "#질문 Template 실행 => 모델 실행\n",
        "_inputs = RunnableParallel(\n",
        "    standalone_question=RunnablePassthrough.assign(\n",
        "        chat_history=lambda x: get_buffer_string(x[\"chat_history\"])\n",
        "    )\n",
        "    | CONDENSE_QUESTION_PROMPT\n",
        "    | ChatOpenAI(temperature=0)\n",
        "    | StrOutputParser(),\n",
        ")\n",
        "\n",
        "#답변 Template의 변수 지정\n",
        "#standalone_question이라는 변수의 값을 vector db 검색기와 docs의 context에서 찾음\n",
        "#찾은 standalone_question 값을 질문으로 반환\n",
        "_context = {\n",
        "    \"context\": itemgetter(\"standalone_question\") | retriever | _combine_documents,\n",
        "    \"question\": lambda x: x[\"standalone_question\"],\n",
        "}\n",
        "\n",
        "#QA Chain으로 연결\n",
        "#input Chain 결과값을 context에 적용하고, context의 변수를 answer 프롬프트에 적용하여 해당 프롬프트로 모델을 실행함\n",
        "conversational_qa_chain = _inputs | _context | ANSWER_PROMPT | ChatOpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bg3qQY9hEiCh",
        "outputId": "897c87da-c8f9-4b84-a652-ac2bdd907e9e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Harrison worked at Kensho.', response_metadata={'finish_reason': 'stop', 'logprobs': None})"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# input prompt에 질문 전달\n",
        "conversational_qa_chain.invoke(\n",
        "    {\n",
        "        \"question\": \"where did harrison work?\",\n",
        "        \"chat_history\": [],\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5aBYq8WEoe9",
        "outputId": "7dfb6e49-453c-418c-8caa-7044d638dcbc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Harrison worked at Kensho.', response_metadata={'finish_reason': 'stop', 'logprobs': None})"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#채팅 히스토리에 데이터를 추가하여 retriever와 함께 context로 사용\n",
        "conversational_qa_chain.invoke(\n",
        "    {\n",
        "        \"question\": \"where did he work?\",\n",
        "        \"chat_history\": [\n",
        "            HumanMessage(content=\"Who wrote this notebook?\"),\n",
        "            AIMessage(content=\"Harrison\"),\n",
        "        ],\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gimt-PpE8Bj"
      },
      "source": [
        "## **With Memory and returning source documents**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "메모리에 대화 내용을 저장하고 답변에 포함된 문서의 직접적인 소스를 반환함\n",
        "=> 결과 값이 어디에서 도출되었는지 확인할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzNYkHeME-JF",
        "outputId": "52f3ec8c-fd80-49cc-c923-0938e91457ac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'answer': AIMessage(content='Harrison worked at Kensho.', response_metadata={'finish_reason': 'stop', 'logprobs': None}),\n",
              " 'docs': [Document(page_content='harrison worked at kensho')]}"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "#대화 내용을 기억할 수 있도록 Memory 설정\n",
        "#question / answer 형태의 메세지로 memory 반환\n",
        "memory = ConversationBufferMemory(\n",
        "    return_messages=True, output_key=\"answer\", input_key=\"question\"\n",
        ")\n",
        "\n",
        "# First we add a step to load memory\n",
        "# This adds a \"memory\" key to the input object\n",
        "# chat_history의 값을 미리 할당합니다. 이때 history의 값을 가지고 옵니다.\n",
        "loaded_memory = RunnablePassthrough.assign(\n",
        "    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"),\n",
        ")\n",
        "\n",
        "# Now we calculate the standalone question\n",
        "# standalone_question은 qustion과 chat_history로, qustion은 질문, chat_history는 과거의 메모리로 부터 가져옵니다.\n",
        "# CONDENSE_QUSTION_PROMPT의 standalone_question 부분에 해당 값이 할당되어 모델에 적용됩니다.\n",
        "standalone_question = {\n",
        "    \"standalone_question\": {\n",
        "        \"question\": lambda x: x[\"question\"],\n",
        "        \"chat_history\": lambda x: get_buffer_string(x[\"chat_history\"]),\n",
        "    }\n",
        "    | CONDENSE_QUESTION_PROMPT\n",
        "    | ChatOpenAI(temperature=0)\n",
        "    | StrOutputParser(),\n",
        "}\n",
        "\n",
        "# Now we retrieve the documents\n",
        "# standalone_question 의 값과 vector db 검색기를 문서로 설정합니다.\n",
        "# 메모리의 chat_history 값이 질문과 함께 문서로 사용됩니다.\n",
        "# 질문은 standalone_question에서 받아옵니다.\n",
        "retrieved_documents = {\n",
        "    \"docs\": itemgetter(\"standalone_question\") | retriever,\n",
        "    \"question\": lambda x: x[\"standalone_question\"],\n",
        "}\n",
        "\n",
        "# Now we construct the inputs for the final prompt\n",
        "# 새로운 문서 값을 context로 사용하여 질문에 답변을 할 수 있도록 input 값으로 지정합니다.\n",
        "final_inputs = {\n",
        "    \"context\": lambda x: _combine_documents(x[\"docs\"]),\n",
        "    \"question\": itemgetter(\"question\"),\n",
        "}\n",
        "\n",
        "# And finally, we do the part that returns the answers\n",
        "# 결과 프롬프트에 최종 입력값을 집어 넣어 모델을 실행합니다.\n",
        "answer = {\n",
        "    \"answer\": final_inputs | ANSWER_PROMPT | ChatOpenAI(),\n",
        "    \"docs\": itemgetter(\"docs\"),\n",
        "}\n",
        "\n",
        "# And now we put it all together!\n",
        "# Chain을 생성합니다.\n",
        "final_chain = loaded_memory | standalone_question | retrieved_documents | answer\n",
        "\n",
        "# Chain을 실행합니다.\n",
        "inputs = {\"question\": \"where did harrison work?\"}\n",
        "result = final_chain.invoke(inputs)\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "0mEcvoc1FYd1"
      },
      "outputs": [],
      "source": [
        "# Note that the memory does not save automatically\n",
        "# This will be improved in the future\n",
        "# For now you need to save it yourself\n",
        "# 메모리를 저장합니다.\n",
        "memory.save_context(inputs, {\"answer\": result[\"answer\"].content})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yz5CQ6VFce-",
        "outputId": "418a72ac-093a-4ea0-83d4-d55c398ad5be"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='where did harrison work?'),\n",
              "  AIMessage(content='Harrison worked at Kensho.')]}"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#저장된 메모리를 불러옵니다.\n",
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0GW33h5Fo1n",
        "outputId": "2f4dc790-dd78-416e-c80c-5eeb1e080255"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'answer': AIMessage(content='Harrison really worked at Kensho.', response_metadata={'finish_reason': 'stop', 'logprobs': None}),\n",
              " 'docs': [Document(page_content='harrison worked at kensho')]}"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs = {\"question\": \"but where did he really work?\"}\n",
        "result = final_chain.invoke(inputs)\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "qW0bzGCyFxDh"
      },
      "outputs": [],
      "source": [
        "memory.save_context(inputs, {\"answer\": result[\"answer\"].content})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pearJdzJFz01",
        "outputId": "b6d187c9-da75-45ce-accd-44ef002a690b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='where did harrison work?'),\n",
              "  AIMessage(content='Harrison worked at Kensho.'),\n",
              "  HumanMessage(content='but where did he really work?'),\n",
              "  AIMessage(content='Harrison really worked at Kensho.')]}"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
