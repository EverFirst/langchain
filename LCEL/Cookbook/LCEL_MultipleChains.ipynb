{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Multiple chains"
      ],
      "metadata": {
        "id": "259M-8OXmQD7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "4Vahr4cWmPN6"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --quiet langchain langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"]      = \"sk-*******************************************************\""
      ],
      "metadata": {
        "id": "86kvSCFwnI2s"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "prompt1 = ChatPromptTemplate.from_template(\"what is the city {person} is from?\")\n",
        "prompt2 = ChatPromptTemplate.from_template(\n",
        "    \"what country is the city {city} in? respond in {language}\"\n",
        ")\n",
        "\n",
        "model = ChatOpenAI()\n",
        "\n",
        "chain1 = prompt1 | model | StrOutputParser()\n",
        "\n",
        "chain2 = (\n",
        "    {\"city\": chain1, \"language\": itemgetter(\"language\")}\n",
        "    | prompt2\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "chain2.invoke({\"person\": \"obama\", \"language\": \"korean\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "G0qTpQ-JB3GQ",
        "outputId": "6bbd2ce9-cf48-47fc-c6cf-0ccb94d5fe7a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'시카고는 일리노이 주에 위치한 도시입니다.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "prompt1 = ChatPromptTemplate.from_template(\n",
        "    \"generate a {attribute} color. Return the name of the color and nothing else:\"\n",
        ")\n",
        "prompt2 = ChatPromptTemplate.from_template(\n",
        "    \"what is a fruit of color: {color}. Return the name of the fruit and nothing else:\"\n",
        ")\n",
        "prompt3 = ChatPromptTemplate.from_template(\n",
        "    \"what is a country with a flag that has the color: {color}. Return the name of the country and nothing else:\"\n",
        ")\n",
        "prompt4 = ChatPromptTemplate.from_template(\n",
        "    \"What is the color of {fruit} and the flag of {country}?\"\n",
        ")\n",
        "\n",
        "model_parser = model | StrOutputParser()\n",
        "\n",
        "color_generator = (\n",
        "    {\"attribute\": RunnablePassthrough()} | prompt1 | {\"color\": model_parser}\n",
        ")\n",
        "color_to_fruit = prompt2 | model_parser\n",
        "color_to_country = prompt3 | model_parser\n",
        "question_generator = (\n",
        "    color_generator | {\"fruit\": color_to_fruit, \"country\": color_to_country} | prompt4\n",
        ")\n",
        "\n",
        "prompt = question_generator.invoke(\"warm\")\n",
        "model.invoke(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVuDC5YvINYJ",
        "outputId": "74d27827-614a-48df-cf90-df1f4c0d3f70"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='The color of orange is orange, and the flag of Cyprus is white with a copper-colored map of the island in the center.', response_metadata={'finish_reason': 'stop', 'logprobs': None})"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Branching and Merging"
      ],
      "metadata": {
        "id": "hDPMPwQUI3MY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "planner = (\n",
        "    ChatPromptTemplate.from_template(\"Generate an argument about: {input}\")\n",
        "    | ChatOpenAI()\n",
        "    | StrOutputParser()\n",
        "    | {\"base_response\": RunnablePassthrough()}\n",
        ")\n",
        "\n",
        "arguments_for = (\n",
        "    ChatPromptTemplate.from_template(\n",
        "        \"List the pros or positive aspects of {base_response}\"\n",
        "    )\n",
        "    | ChatOpenAI()\n",
        "    | StrOutputParser()\n",
        ")\n",
        "arguments_against = (\n",
        "    ChatPromptTemplate.from_template(\n",
        "        \"List the cons or negative aspects of {base_response}\"\n",
        "    )\n",
        "    | ChatOpenAI()\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "final_responder = (\n",
        "    ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"ai\", \"{original_response}\"),\n",
        "            (\"human\", \"Pros:\\n{results_1}\\n\\nCons:\\n{results_2}\"),\n",
        "            (\"system\", \"Generate a final response given the critique\"),\n",
        "        ]\n",
        "    )\n",
        "    | ChatOpenAI()\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "chain = (\n",
        "    planner\n",
        "    | {\n",
        "        \"results_1\": arguments_for,\n",
        "        \"results_2\": arguments_against,\n",
        "        \"original_response\": itemgetter(\"base_response\"),\n",
        "    }\n",
        "    | final_responder\n",
        ")\n",
        "\n",
        "chain.invoke({\"input\": \"scrum\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "eUH0CknII4c5",
        "outputId": "f8521794-2c11-404f-9a6c-51affb7af908"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'While Scrum offers numerous benefits such as promoting collaboration, adaptability, and continuous improvement, there are indeed some potential drawbacks to consider. These include the need for a high level of commitment from team members, challenges with large or complex projects, potential undefined roles and responsibilities, scope creep risks, and dependencies on strong team dynamics.\\n\\nTo address these concerns and maximize the effectiveness of Scrum, it is crucial to ensure clear communication, well-defined roles, and a strong commitment from all team members. Additionally, proper project planning and monitoring can help mitigate scope creep and adapt Scrum practices to suit the specific needs of the project. By proactively addressing these potential challenges, teams can harness the power of Scrum to deliver high-quality results efficiently and effectively.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    }
  ]
}