{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgHeElS0UAXP"
      },
      "source": [
        "# Set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_f4kdIaV5GtR"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --quiet langchain langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Xi78Gmm85ceP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = 'sk-********************************************************'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCXF7EisO1WN"
      },
      "source": [
        "# Memory management"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJ28kKZ9PoBz"
      },
      "source": [
        "## Message passing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "가장 간단한 형태의 메모리는 채팅 기록 메시지를 체인으로 전달하는 것입니다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "D3EZRMpC6Bz8"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chat = ChatOpenAI(model=\"gpt-3.5-turbo-1106\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IS03RO5GO4bU",
        "outputId": "7222e564-4402-440f-d3b2-dd437559a61b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='I said \"J\\'adore la programmation,\" which means \"I love programming\" in French.', response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 61, 'total_tokens': 82}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': 'fp_9dd82289bf', 'finish_reason': 'stop', 'logprobs': None})"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt | chat\n",
        "\n",
        "chain.invoke(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            HumanMessage(\n",
        "                content=\"Translate this sentence from English to French: I love programming.\"\n",
        "            ),\n",
        "            AIMessage(content=\"J'adore la programmation.\"),\n",
        "            HumanMessage(content=\"What did you just say?\"),\n",
        "        ],\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eHZy6_ZPsji"
      },
      "source": [
        "## Chat history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "LangChain에 내장된 `ChatMessageHistory` 사용하여 메시지를 저장하고 로드할 수도 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hap6BgFMPQ3b",
        "outputId": "797f1c2d-e935-450a-9a0d-60d2c59c6d13"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content='Translate this sentence from English to French: I love programming.'),\n",
              " AIMessage(content=\"J'adore la programmation.\")]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.memory import ChatMessageHistory\n",
        "\n",
        "demo_ephemeral_chat_history = ChatMessageHistory()\n",
        "\n",
        "demo_ephemeral_chat_history.add_user_message(\n",
        "    \"Translate this sentence from English to French: I love programming.\"\n",
        ")\n",
        "\n",
        "demo_ephemeral_chat_history.add_ai_message(\"J'adore la programmation.\")\n",
        "\n",
        "demo_ephemeral_chat_history.messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "체인의 대화 순서를 저장하는 데 직접 사용할 수 있습니다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yueu8QJGRdbe",
        "outputId": "e391b92e-e5e6-41c1-95e1-76ddbc047e8b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='You just asked me to translate the sentence \"I love programming\" from English to French.', response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 74, 'total_tokens': 92}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': 'fp_f93e21ed76', 'finish_reason': 'stop', 'logprobs': None})"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "demo_ephemeral_chat_history = ChatMessageHistory()\n",
        "\n",
        "input1 = \"Translate this sentence from English to French: I love programming.\"\n",
        "\n",
        "demo_ephemeral_chat_history.add_user_message(input1)\n",
        "\n",
        "response = chain.invoke(\n",
        "    {\n",
        "        \"messages\": demo_ephemeral_chat_history.messages,\n",
        "    }\n",
        ")\n",
        "\n",
        "demo_ephemeral_chat_history.add_ai_message(response)\n",
        "\n",
        "input2 = \"What did I just ask you?\"\n",
        "\n",
        "demo_ephemeral_chat_history.add_user_message(input2)\n",
        "\n",
        "chain.invoke(\n",
        "    {\n",
        "        \"messages\": demo_ephemeral_chat_history.messages,\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpVtvnaWSUUi"
      },
      "source": [
        "## Automatic history management"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`RunnableWithMessageHistory는` 또 다른 Runnable을 래핑하고 이에 대한 채팅 메시지 기록을 관리합니다. 채팅 메시지 기록을 읽고 업데이트하는 일을 담당합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "a_nyBIgWSKBZ"
      },
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt | chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UlQkwI3MS5Hl"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "demo_ephemeral_chat_history_for_chain = ChatMessageHistory()\n",
        "\n",
        "chain_with_message_history = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    lambda session_id: demo_ephemeral_chat_history_for_chain,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"chat_history\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5R_NAPQYS6Y8",
        "outputId": "adea438b-e9cf-4e6b-f5c1-11c8f8d0f177"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='The translation of \"I love programming\" to French is \"J\\'adore la programmation.\"', response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 39, 'total_tokens': 59}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': 'fp_9dd82289bf', 'finish_reason': 'stop', 'logprobs': None})"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain_with_message_history.invoke(\n",
        "    {\"input\": \"Translate this sentence from English to French: I love programming.\"},\n",
        "    {\"configurable\": {\"session_id\": \"unused\"}},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edcVcKiRTC1s",
        "outputId": "d9f04083-1b06-4fd4-a638-81c090fee331"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='You just asked me to translate the sentence \"I love programming\" from English to French.', response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 74, 'total_tokens': 92}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': 'fp_9dd82289bf', 'finish_reason': 'stop', 'logprobs': None})"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain_with_message_history.invoke(\n",
        "    {\"input\": \"What did I just ask you?\"}, {\"configurable\": {\"session_id\": \"unused\"}}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0mu4-2YTiV8"
      },
      "source": [
        "## Modifying chat history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "저장된 채팅 메시지를 수정하면 챗봇이 다양한 상황을 처리하는 데 도움이 될 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wC2PLahITpBe"
      },
      "source": [
        "### Trimming messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1PYKR4yXjY_"
      },
      "source": [
        "- LLM과 채팅 모델에는 제한된 컨텍스트 창이 있으며, 제한을 직접적으로 맞추지 않더라도 모델이 처리해야 할 주의 분산의 양을 제한\n",
        "\n",
        "- 가장 최근 n개의 메시지만 로드하고 저장"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTioMCwzTZmf",
        "outputId": "5b549c76-69d5-4e66-cc42-553b3d5b7755"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content=\"Hey there! I'm Nemo.\"),\n",
              " AIMessage(content='Hello!'),\n",
              " HumanMessage(content='How are you today?'),\n",
              " AIMessage(content='Fine thanks!')]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "demo_ephemeral_chat_history = ChatMessageHistory()\n",
        "\n",
        "demo_ephemeral_chat_history.add_user_message(\"Hey there! I'm Nemo.\")\n",
        "demo_ephemeral_chat_history.add_ai_message(\"Hello!\")\n",
        "demo_ephemeral_chat_history.add_user_message(\"How are you today?\")\n",
        "demo_ephemeral_chat_history.add_ai_message(\"Fine thanks!\")\n",
        "\n",
        "demo_ephemeral_chat_history.messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwxCpdyUT5X5",
        "outputId": "a8eb8c63-3783-4106-d068-024401a6e4b0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Your name is Nemo.', response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 66, 'total_tokens': 72}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': 'fp_9dd82289bf', 'finish_reason': 'stop', 'logprobs': None})"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt | chat\n",
        "\n",
        "chain_with_message_history = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    lambda session_id: demo_ephemeral_chat_history,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"chat_history\",\n",
        ")\n",
        "\n",
        "chain_with_message_history.invoke(\n",
        "    {\"input\": \"What's my name?\"},\n",
        "    {\"configurable\": {\"session_id\": \"unused\"}},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "E6T4ByYYT8eP"
      },
      "outputs": [],
      "source": [
        "# 체인에 전달되는 메시지 수를 가장 최근 메시지 2개로만 잘라내기\n",
        "\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "\n",
        "def trim_messages(chain_input):\n",
        "    stored_messages = demo_ephemeral_chat_history.messages\n",
        "    if len(stored_messages) <= 2:\n",
        "        return False\n",
        "\n",
        "    demo_ephemeral_chat_history.clear()\n",
        "\n",
        "    for message in stored_messages[-2:]:\n",
        "        demo_ephemeral_chat_history.add_message(message)\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "chain_with_trimming = (\n",
        "    RunnablePassthrough.assign(messages_trimmed=trim_messages)\n",
        "    | chain_with_message_history\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFkIiHKLX5fQ",
        "outputId": "9c7a59d0-67c1-4fec-ad24-dbf6bc2632d0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"P. Sherman's address is 42 Wallaby Way, Sydney.\", response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 53, 'total_tokens': 67}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': 'fp_f93e21ed76', 'finish_reason': 'stop', 'logprobs': None})"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain_with_trimming.invoke(\n",
        "    {\"input\": \"Where does P. Sherman live?\"},\n",
        "    {\"configurable\": {\"session_id\": \"unused\"}},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYRgsF8kZpeR",
        "outputId": "14f7003e-09bb-4a48-aeb2-6f4d8dada2ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content=\"What's my name?\"),\n",
              " AIMessage(content='Your name is Nemo.', response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 66, 'total_tokens': 72}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': 'fp_9dd82289bf', 'finish_reason': 'stop', 'logprobs': None}),\n",
              " HumanMessage(content='Where does P. Sherman live?'),\n",
              " AIMessage(content=\"P. Sherman's address is 42 Wallaby Way, Sydney.\", response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 53, 'total_tokens': 67}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': 'fp_f93e21ed76', 'finish_reason': 'stop', 'logprobs': None})]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "demo_ephemeral_chat_history.messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbhXD-cQaYhQ"
      },
      "source": [
        "### Summary memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "LLM 호출을 사용하여 체인을 호출하기 전에 대화 요약을 생성할 수 있습니다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMHjjOY6ZsKu",
        "outputId": "8eb95cd6-6070-426c-954b-93b0beb53e80"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content=\"Hey there! I'm Nemo.\"),\n",
              " AIMessage(content='Hello!'),\n",
              " HumanMessage(content='How are you today?'),\n",
              " AIMessage(content='Fine thanks!')]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "demo_ephemeral_chat_history = ChatMessageHistory()\n",
        "\n",
        "demo_ephemeral_chat_history.add_user_message(\"Hey there! I'm Nemo.\")\n",
        "demo_ephemeral_chat_history.add_ai_message(\"Hello!\")\n",
        "demo_ephemeral_chat_history.add_user_message(\"How are you today?\")\n",
        "demo_ephemeral_chat_history.add_ai_message(\"Fine thanks!\")\n",
        "\n",
        "demo_ephemeral_chat_history.messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "I5gSxDTJa4fa"
      },
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Answer all questions to the best of your ability. The provided chat history includes facts about the user you are speaking with.\",\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        (\"user\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt | chat\n",
        "\n",
        "chain_with_message_history = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    lambda session_id: demo_ephemeral_chat_history,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"chat_history\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ra4f29lRa8BO"
      },
      "outputs": [],
      "source": [
        "def summarize_messages(chain_input):\n",
        "    stored_messages = demo_ephemeral_chat_history.messages\n",
        "    if len(stored_messages) == 0:\n",
        "        return False\n",
        "    summarization_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "            (\n",
        "                \"user\",\n",
        "                \"Distill the above chat messages into a single summary message. Include as many specific details as you can.\",\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "    summarization_chain = summarization_prompt | chat\n",
        "\n",
        "    summary_message = summarization_chain.invoke({\"chat_history\": stored_messages})\n",
        "\n",
        "    demo_ephemeral_chat_history.clear()\n",
        "\n",
        "    demo_ephemeral_chat_history.add_message(summary_message)\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "chain_with_summarization = (\n",
        "    RunnablePassthrough.assign(messages_summarized=summarize_messages)\n",
        "    | chain_with_message_history\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghAB33kibEtK",
        "outputId": "a2027d2c-3704-4bcd-f5d7-be33b81f268b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Your name is Nemo.', response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 101, 'total_tokens': 107}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': 'fp_9dd82289bf', 'finish_reason': 'stop', 'logprobs': None})"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain_with_summarization.invoke(\n",
        "    {\"input\": \"What did I say my name was?\"},\n",
        "    {\"configurable\": {\"session_id\": \"unused\"}},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8IZ0VRUbNah",
        "outputId": "7e15d3e3-4439-4778-b335-7e092536f544"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[AIMessage(content='The conversation is between two individuals, Nemo and an unnamed person. Nemo introduces himself and the other person responds with a greeting. The other person then asks how Nemo is doing, to which Nemo responds that he is fine.', response_metadata={'token_usage': {'completion_tokens': 48, 'prompt_tokens': 62, 'total_tokens': 110}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': 'fp_9dd82289bf', 'finish_reason': 'stop', 'logprobs': None}),\n",
              " HumanMessage(content='What did I say my name was?'),\n",
              " AIMessage(content='Your name is Nemo.', response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 101, 'total_tokens': 107}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': 'fp_9dd82289bf', 'finish_reason': 'stop', 'logprobs': None})]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "demo_ephemeral_chat_history.messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqqVMDvJdVWJ"
      },
      "source": [
        "# Retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`Retrieval`은 챗봇이 채팅 모델의 훈련 데이터 외부의 데이터로 응답을 강화하는 데 사용하는 일반적인 기술입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYrtTRCFdWOI",
        "outputId": "b4c52f8e-19ff-4b1f-d3f8-473d4b4f9cb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m525.5/525.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.7/105.7 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --quiet langchain langchain-openai chromadb beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "yA0T5DHadZ1y"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = 'sk-********************************************************'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "HiyPeL7_dn30"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chat = ChatOpenAI(model=\"gpt-3.5-turbo-1106\", temperature=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_qQ9UEHdvZA"
      },
      "source": [
        "## Retriever 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`WebBaseLoader`를 사용하여 문서를 로드 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "NCysAOLhduTy"
      },
      "outputs": [],
      "source": [
        "# 문서 로더를 사용하여 문서에서 텍스트를 가져오기\n",
        "\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "loader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")\n",
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`RecursiveCharacterTextSplitter`를 사용하여 텍스트를 분할합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "QK8GqGA0d1-K"
      },
      "outputs": [],
      "source": [
        "# 다음으로 이를 LLM의 컨텍스트 창이 처리할 수 있는 더 작은 청크로 분할\n",
        "\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
        "all_splits = text_splitter.split_documents(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`Chroma`에 임베딩 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "cvemFPyreBHw"
      },
      "outputs": [],
      "source": [
        "# 해당 청크를 벡터 데이터베이스에 저장\n",
        "\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abOFs1uDeKiH",
        "outputId": "1c48a827-089a-42ab-ca3b-5e48314c9d46"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(page_content='go-to open source framework for building with LLMs.LangSmith is developed by LangChain, the company behind the open source LangChain framework.Quick Start‚ÄãTracing: Get started with the tracing quick start.Evaluation: Get started with the evaluation quick start.Next Steps‚ÄãCheck out the following sections to learn more about LangSmith:User Guide: Learn about the workflows LangSmith supports at each stage of the LLM application lifecycle.Setup: Learn how to create an account, obtain an API', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n",
              " Document(page_content='LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n",
              " Document(page_content='Skip to main content\\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith DocsLangChain Python DocsLangChain JS/TS DocsLangSmith API DocsSearchGo to AppLangSmithUser GuideSetupPricing (Coming Soon)Self-HostingTracingEvaluationMonitoringPrompt HubProxyCookbookLangSmithOn this pageLangSmithIntroduction‚ÄãLangSmith is a platform for building production-grade LLM applications.It lets you debug, test, evaluate, and monitor chains and intelligent agents built on any LLM framework and seamlessly integrates with LangChain, the', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n",
              " Document(page_content='key, and configure your environment.Pricing: Learn about the pricing model for LangSmith.Self-Hosting: Learn about self-hosting options for LangSmith.Proxy: Learn about the proxy capabilities of LangSmith.Tracing: Learn about the tracing capabilities of LangSmith.Evaluation: Learn about the evaluation capabilities of LangSmith.Prompt Hub Learn about the Prompt Hub, a prompt management tool built into LangSmith.Additional Resources‚ÄãLangSmith Cookbook: A collection of tutorials and end-to-end', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'})]"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# k: 검색할 청크 수입니다.\n",
        "retriever = vectorstore.as_retriever(k=4)\n",
        "\n",
        "docs = retriever.invoke(\"Can LangSmith help test my LLM applications?\")\n",
        "\n",
        "docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rgag2WP2ecWz"
      },
      "source": [
        "## Document chains"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "LangChain 문서를 반환할 수 있는 리트리버가 있으므로 질문에 답하는 컨텍스트로 사용할 수 있는 체인을 생성합니다.\n",
        "\n",
        "`create_stuff_documents_chain`을 사용하여 Input된 문서를 프롬프트에 입력합니다.\n",
        "\n",
        "이 함수는 컨텍스트 변수가 있는 프롬프트를 전달하고 `MessagesPlaceholder`를 통해 대화 history도 포함시킵니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "mMeZPobyeO1N"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "# Prompt 생성\n",
        "SYSTEM_TEMPLATE = \"\"\"\n",
        "Answer the user's questions based on the below context.\n",
        "If the context doesn't contain any relevant information to the question, don't make something up and just say \"I don't know\":\n",
        "\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\"\"\"\n",
        "\n",
        "question_answering_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            SYSTEM_TEMPLATE,\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Document 체인 생성\n",
        "document_chain = create_stuff_documents_chain(chat, question_answering_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-1pKhly4ei60",
        "outputId": "b517cd4c-e5b1-4399-eaec-b5314378bc1c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Yes, LangSmith can help test your LLM applications. It provides evaluation capabilities for testing and evaluating chains and intelligent agents built on any LLM framework.'"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "document_chain.invoke(\n",
        "    {\n",
        "        \"context\": docs,\n",
        "        \"messages\": [\n",
        "            HumanMessage(content=\"Can LangSmith help test my LLM applications?\")\n",
        "        ],\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "kboiWJcUfB3w",
        "outputId": "a66a9e63-f07d-4412-8fa4-db4920130176"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"I don't know.\""
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 문서 없이 실행\n",
        "\n",
        "document_chain.invoke(\n",
        "    {\n",
        "        \"context\": [],\n",
        "        \"messages\": [\n",
        "            HumanMessage(content=\"Can LangSmith help test my LLM applications?\")\n",
        "        ],\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUazSJiyfe-b"
      },
      "source": [
        "## Retrieval chains"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`Retrieval chains`을 통해 `create_stuff_documents_chain`과 `Retriever`를 결합합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "q_RhkIgifb3j"
      },
      "outputs": [],
      "source": [
        "# 입력 메시지 목록이 주어지면 목록에서 마지막 메시지의 내용을 추출하여 일부 문서를 가져오도록 리트리버에 전달\n",
        "# 해당 문서를 컨텍스트로 문서 체인에 전달하여 최종 응답을 생성\n",
        "\n",
        "from typing import Dict\n",
        "\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "\n",
        "def parse_retriever_input(params: Dict):\n",
        "    return params[\"messages\"][-1].content\n",
        "\n",
        "\n",
        "retrieval_chain = RunnablePassthrough.assign(\n",
        "    context=parse_retriever_input | retriever,\n",
        ").assign(\n",
        "    answer=document_chain,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XXUKLoZflr3",
        "outputId": "135eefa0-ec07-4683-a16e-8adfe102c245"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='Can LangSmith help test my LLM applications?')],\n",
              " 'context': [Document(page_content='go-to open source framework for building with LLMs.LangSmith is developed by LangChain, the company behind the open source LangChain framework.Quick Start‚ÄãTracing: Get started with the tracing quick start.Evaluation: Get started with the evaluation quick start.Next Steps‚ÄãCheck out the following sections to learn more about LangSmith:User Guide: Learn about the workflows LangSmith supports at each stage of the LLM application lifecycle.Setup: Learn how to create an account, obtain an API', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n",
              "  Document(page_content='LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n",
              "  Document(page_content='Skip to main content\\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith DocsLangChain Python DocsLangChain JS/TS DocsLangSmith API DocsSearchGo to AppLangSmithUser GuideSetupPricing (Coming Soon)Self-HostingTracingEvaluationMonitoringPrompt HubProxyCookbookLangSmithOn this pageLangSmithIntroduction‚ÄãLangSmith is a platform for building production-grade LLM applications.It lets you debug, test, evaluate, and monitor chains and intelligent agents built on any LLM framework and seamlessly integrates with LangChain, the', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n",
              "  Document(page_content='key, and configure your environment.Pricing: Learn about the pricing model for LangSmith.Self-Hosting: Learn about self-hosting options for LangSmith.Proxy: Learn about the proxy capabilities of LangSmith.Tracing: Learn about the tracing capabilities of LangSmith.Evaluation: Learn about the evaluation capabilities of LangSmith.Prompt Hub Learn about the Prompt Hub, a prompt management tool built into LangSmith.Additional Resources‚ÄãLangSmith Cookbook: A collection of tutorials and end-to-end', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'})],\n",
              " 'answer': 'Yes, LangSmith can help test your LLM applications. It provides capabilities for evaluation, tracing, and monitoring chains and intelligent agents built on any LLM framework.'}"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieval_chain.invoke(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            HumanMessage(content=\"Can LangSmith help test my LLM applications?\")\n",
        "        ],\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nb5GtRpgnvq"
      },
      "source": [
        "## Query transformation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "위의 `Retrieval chains`은 질문에 답할 수는 있지만 사용자와 대화를 통한 상호작용이 불가능 합니다.\n",
        "\n",
        "Retriever는 주어진 쿼리와 가장 유사한 문서만 끌어오기 때문입니다.\n",
        "\n",
        "이를 해결하기 위해 외부 참조 없이 LLM 없이 쿼리를 독립 실행형 쿼리로 변환할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BKt4mUtgkYT",
        "outputId": "1b47e9f0-77dd-4560-c0ba-9ed15ab54fca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(page_content='LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n",
              " Document(page_content='GuideIntroductionQuick StartNext StepsAdditional ResourcesCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogCopyright ¬© 2024 LangChain, Inc.', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n",
              " Document(page_content='key, and configure your environment.Pricing: Learn about the pricing model for LangSmith.Self-Hosting: Learn about self-hosting options for LangSmith.Proxy: Learn about the proxy capabilities of LangSmith.Tracing: Learn about the tracing capabilities of LangSmith.Evaluation: Learn about the evaluation capabilities of LangSmith.Prompt Hub Learn about the Prompt Hub, a prompt management tool built into LangSmith.Additional Resources‚ÄãLangSmith Cookbook: A collection of tutorials and end-to-end', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n",
              " Document(page_content='Skip to main content\\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith DocsLangChain Python DocsLangChain JS/TS DocsLangSmith API DocsSearchGo to AppLangSmithUser GuideSetupPricing (Coming Soon)Self-HostingTracingEvaluationMonitoringPrompt HubProxyCookbookLangSmithOn this pageLangSmithIntroduction‚ÄãLangSmith is a platform for building production-grade LLM applications.It lets you debug, test, evaluate, and monitor chains and intelligent agents built on any LLM framework and seamlessly integrates with LangChain, the', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'})]"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retriever.invoke(\"Tell me more!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSyX_t8JioDf",
        "outputId": "c9e633bf-7fc8-47d9-c08a-ac74b0700421"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='\"LangSmith LLM application testing and evaluation\"', response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 145, 'total_tokens': 155}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': 'fp_9dd82289bf', 'finish_reason': 'stop', 'logprobs': None})"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "\n",
        "query_transform_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "        (\n",
        "            \"user\",\n",
        "            \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation. Only respond with the query, nothing else.\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "query_transformation_chain = query_transform_prompt | chat\n",
        "\n",
        "query_transformation_chain.invoke(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            HumanMessage(content=\"Can LangSmith help test my LLM applications?\"),\n",
        "            AIMessage(\n",
        "                content=\"Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.\"\n",
        "            ),\n",
        "            HumanMessage(content=\"Tell me more!\"),\n",
        "        ],\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "AWc216GejZL_"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableBranch\n",
        "\n",
        "query_transforming_retriever_chain = RunnableBranch(\n",
        "    (\n",
        "        lambda x: len(x.get(\"messages\", [])) == 1,\n",
        "        # If only one message, then we just pass that message's content to retriever\n",
        "        (lambda x: x[\"messages\"][-1].content) | retriever,\n",
        "    ),\n",
        "    # If messages, then we pass inputs to LLM chain to transform the query, then pass to retriever\n",
        "    query_transform_prompt | chat | StrOutputParser() | retriever,\n",
        ").with_config(run_name=\"chat_retriever_chain\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "lbq7SisOjilw"
      },
      "outputs": [],
      "source": [
        "SYSTEM_TEMPLATE = \"\"\"\n",
        "Answer the user's questions based on the below context.\n",
        "If the context doesn't contain any relevant information to the question, don't make something up and just say \"I don't know\":\n",
        "\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\"\"\"\n",
        "\n",
        "question_answering_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            SYSTEM_TEMPLATE,\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "document_chain = create_stuff_documents_chain(chat, question_answering_prompt)\n",
        "\n",
        "conversational_retrieval_chain = RunnablePassthrough.assign(\n",
        "    context=query_transforming_retriever_chain,\n",
        ").assign(\n",
        "    answer=document_chain,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1CEGaW9j_5d",
        "outputId": "6738ccd5-2af8-451f-96c9-b38ad32bd664"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='Can LangSmith help test my LLM applications?')],\n",
              " 'context': [Document(page_content='go-to open source framework for building with LLMs.LangSmith is developed by LangChain, the company behind the open source LangChain framework.Quick Start‚ÄãTracing: Get started with the tracing quick start.Evaluation: Get started with the evaluation quick start.Next Steps‚ÄãCheck out the following sections to learn more about LangSmith:User Guide: Learn about the workflows LangSmith supports at each stage of the LLM application lifecycle.Setup: Learn how to create an account, obtain an API', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n",
              "  Document(page_content='LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n",
              "  Document(page_content='Skip to main content\\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith DocsLangChain Python DocsLangChain JS/TS DocsLangSmith API DocsSearchGo to AppLangSmithUser GuideSetupPricing (Coming Soon)Self-HostingTracingEvaluationMonitoringPrompt HubProxyCookbookLangSmithOn this pageLangSmithIntroduction‚ÄãLangSmith is a platform for building production-grade LLM applications.It lets you debug, test, evaluate, and monitor chains and intelligent agents built on any LLM framework and seamlessly integrates with LangChain, the', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n",
              "  Document(page_content='key, and configure your environment.Pricing: Learn about the pricing model for LangSmith.Self-Hosting: Learn about self-hosting options for LangSmith.Proxy: Learn about the proxy capabilities of LangSmith.Tracing: Learn about the tracing capabilities of LangSmith.Evaluation: Learn about the evaluation capabilities of LangSmith.Prompt Hub Learn about the Prompt Hub, a prompt management tool built into LangSmith.Additional Resources‚ÄãLangSmith Cookbook: A collection of tutorials and end-to-end', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'})],\n",
              " 'answer': 'Yes, LangSmith can help you test your LLM applications. It provides capabilities for debugging, testing, evaluating, and monitoring chains and intelligent agents built on any LLM framework. You can get started with the evaluation quick start to learn more about how LangSmith supports testing LLM applications.'}"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversational_retrieval_chain.invoke(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            HumanMessage(content=\"Can LangSmith help test my LLM applications?\"),\n",
        "        ]\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOiAo4KZkC6S",
        "outputId": "85fae410-4ba6-4218-c5fd-bddb48fdb538"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='Can LangSmith help test my LLM applications?'),\n",
              "  AIMessage(content='Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.'),\n",
              "  HumanMessage(content='Tell me more!')],\n",
              " 'context': [Document(page_content='LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n",
              "  Document(page_content='go-to open source framework for building with LLMs.LangSmith is developed by LangChain, the company behind the open source LangChain framework.Quick Start‚ÄãTracing: Get started with the tracing quick start.Evaluation: Get started with the evaluation quick start.Next Steps‚ÄãCheck out the following sections to learn more about LangSmith:User Guide: Learn about the workflows LangSmith supports at each stage of the LLM application lifecycle.Setup: Learn how to create an account, obtain an API', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n",
              "  Document(page_content='Skip to main content\\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith DocsLangChain Python DocsLangChain JS/TS DocsLangSmith API DocsSearchGo to AppLangSmithUser GuideSetupPricing (Coming Soon)Self-HostingTracingEvaluationMonitoringPrompt HubProxyCookbookLangSmithOn this pageLangSmithIntroduction‚ÄãLangSmith is a platform for building production-grade LLM applications.It lets you debug, test, evaluate, and monitor chains and intelligent agents built on any LLM framework and seamlessly integrates with LangChain, the', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n",
              "  Document(page_content='key, and configure your environment.Pricing: Learn about the pricing model for LangSmith.Self-Hosting: Learn about self-hosting options for LangSmith.Proxy: Learn about the proxy capabilities of LangSmith.Tracing: Learn about the tracing capabilities of LangSmith.Evaluation: Learn about the evaluation capabilities of LangSmith.Prompt Hub Learn about the Prompt Hub, a prompt management tool built into LangSmith.Additional Resources‚ÄãLangSmith Cookbook: A collection of tutorials and end-to-end', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'})],\n",
              " 'answer': \"LangSmith is a platform designed specifically for building production-grade LLM applications. It provides capabilities for debugging, testing, evaluating, and monitoring chains and intelligent agents built on any LLM framework. Additionally, LangSmith seamlessly integrates with LangChain, the key, and allows you to configure your environment. It also offers features such as tracing, evaluation, and a Prompt Hub, which is a prompt management tool built into LangSmith. If you're interested in learning more, you can explore the User Guide to understand the workflows supported at each stage of the LLM application lifecycle, as well as the Setup section to learn how to create an account and obtain an API key.\"}"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversational_retrieval_chain.invoke(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            HumanMessage(content=\"Can LangSmith help test my LLM applications?\"),\n",
        "            AIMessage(\n",
        "                content=\"Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.\"\n",
        "            ),\n",
        "            HumanMessage(content=\"Tell me more!\"),\n",
        "        ],\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1x-NeaskSFW"
      },
      "source": [
        "## Streaming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "이 Chain은 LCEL로 구성되므로 `Stream()`메서드를 사용할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_YVwQm6kFMY",
        "outputId": "4484184e-e033-4952-be51-088b88f718b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'messages': [HumanMessage(content='Can LangSmith help test my LLM applications?'), AIMessage(content='Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.'), HumanMessage(content='Tell me more!')]}\n",
            "{'context': [Document(page_content='LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}), Document(page_content='go-to open source framework for building with LLMs.LangSmith is developed by LangChain, the company behind the open source LangChain framework.Quick Start‚ÄãTracing: Get started with the tracing quick start.Evaluation: Get started with the evaluation quick start.Next Steps‚ÄãCheck out the following sections to learn more about LangSmith:User Guide: Learn about the workflows LangSmith supports at each stage of the LLM application lifecycle.Setup: Learn how to create an account, obtain an API', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}), Document(page_content='Skip to main content\\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith DocsLangChain Python DocsLangChain JS/TS DocsLangSmith API DocsSearchGo to AppLangSmithUser GuideSetupPricing (Coming Soon)Self-HostingTracingEvaluationMonitoringPrompt HubProxyCookbookLangSmithOn this pageLangSmithIntroduction‚ÄãLangSmith is a platform for building production-grade LLM applications.It lets you debug, test, evaluate, and monitor chains and intelligent agents built on any LLM framework and seamlessly integrates with LangChain, the', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}), Document(page_content='key, and configure your environment.Pricing: Learn about the pricing model for LangSmith.Self-Hosting: Learn about self-hosting options for LangSmith.Proxy: Learn about the proxy capabilities of LangSmith.Tracing: Learn about the tracing capabilities of LangSmith.Evaluation: Learn about the evaluation capabilities of LangSmith.Prompt Hub Learn about the Prompt Hub, a prompt management tool built into LangSmith.Additional Resources‚ÄãLangSmith Cookbook: A collection of tutorials and end-to-end', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'})]}\n",
            "{'answer': ''}\n",
            "{'answer': 'Lang'}\n",
            "{'answer': 'Smith'}\n",
            "{'answer': ' is'}\n",
            "{'answer': ' a'}\n",
            "{'answer': ' platform'}\n",
            "{'answer': ' designed'}\n",
            "{'answer': ' for'}\n",
            "{'answer': ' building'}\n",
            "{'answer': ' production'}\n",
            "{'answer': '-grade'}\n",
            "{'answer': ' L'}\n",
            "{'answer': 'LM'}\n",
            "{'answer': ' ('}\n",
            "{'answer': 'Language'}\n",
            "{'answer': ' Model'}\n",
            "{'answer': ')'}\n",
            "{'answer': ' applications'}\n",
            "{'answer': '.'}\n",
            "{'answer': ' It'}\n",
            "{'answer': ' offers'}\n",
            "{'answer': ' capabilities'}\n",
            "{'answer': ' to'}\n",
            "{'answer': ' debug'}\n",
            "{'answer': ','}\n",
            "{'answer': ' test'}\n",
            "{'answer': ','}\n",
            "{'answer': ' evaluate'}\n",
            "{'answer': ','}\n",
            "{'answer': ' and'}\n",
            "{'answer': ' monitor'}\n",
            "{'answer': ' chains'}\n",
            "{'answer': ' and'}\n",
            "{'answer': ' intelligent'}\n",
            "{'answer': ' agents'}\n",
            "{'answer': ' built'}\n",
            "{'answer': ' on'}\n",
            "{'answer': ' any'}\n",
            "{'answer': ' L'}\n",
            "{'answer': 'LM'}\n",
            "{'answer': ' framework'}\n",
            "{'answer': '.'}\n",
            "{'answer': ' It'}\n",
            "{'answer': ' seamlessly'}\n",
            "{'answer': ' integrates'}\n",
            "{'answer': ' with'}\n",
            "{'answer': ' Lang'}\n",
            "{'answer': 'Chain'}\n",
            "{'answer': ','}\n",
            "{'answer': ' the'}\n",
            "{'answer': ' open'}\n",
            "{'answer': ' source'}\n",
            "{'answer': ' framework'}\n",
            "{'answer': ' developed'}\n",
            "{'answer': ' by'}\n",
            "{'answer': ' Lang'}\n",
            "{'answer': 'Chain'}\n",
            "{'answer': '.'}\n",
            "{'answer': ' \\n\\n'}\n",
            "{'answer': 'In'}\n",
            "{'answer': ' terms'}\n",
            "{'answer': ' of'}\n",
            "{'answer': ' testing'}\n",
            "{'answer': ','}\n",
            "{'answer': ' Lang'}\n",
            "{'answer': 'Smith'}\n",
            "{'answer': ' provides'}\n",
            "{'answer': ' tracing'}\n",
            "{'answer': ' and'}\n",
            "{'answer': ' evaluation'}\n",
            "{'answer': ' capabilities'}\n",
            "{'answer': '.'}\n",
            "{'answer': ' Tr'}\n",
            "{'answer': 'acing'}\n",
            "{'answer': ' allows'}\n",
            "{'answer': ' you'}\n",
            "{'answer': ' to'}\n",
            "{'answer': ' trace'}\n",
            "{'answer': ' the'}\n",
            "{'answer': ' execution'}\n",
            "{'answer': ' of'}\n",
            "{'answer': ' your'}\n",
            "{'answer': ' L'}\n",
            "{'answer': 'LM'}\n",
            "{'answer': ' applications'}\n",
            "{'answer': ','}\n",
            "{'answer': ' while'}\n",
            "{'answer': ' evaluation'}\n",
            "{'answer': ' enables'}\n",
            "{'answer': ' you'}\n",
            "{'answer': ' to'}\n",
            "{'answer': ' test'}\n",
            "{'answer': ' and'}\n",
            "{'answer': ' evaluate'}\n",
            "{'answer': ' the'}\n",
            "{'answer': ' performance'}\n",
            "{'answer': ' of'}\n",
            "{'answer': ' your'}\n",
            "{'answer': ' language'}\n",
            "{'answer': ' models'}\n",
            "{'answer': '.'}\n",
            "{'answer': ' Additionally'}\n",
            "{'answer': ','}\n",
            "{'answer': ' Lang'}\n",
            "{'answer': 'Smith'}\n",
            "{'answer': ' offers'}\n",
            "{'answer': ' a'}\n",
            "{'answer': ' Prompt'}\n",
            "{'answer': ' Hub'}\n",
            "{'answer': ','}\n",
            "{'answer': ' which'}\n",
            "{'answer': ' is'}\n",
            "{'answer': ' a'}\n",
            "{'answer': ' prompt'}\n",
            "{'answer': ' management'}\n",
            "{'answer': ' tool'}\n",
            "{'answer': ' built'}\n",
            "{'answer': ' into'}\n",
            "{'answer': ' the'}\n",
            "{'answer': ' platform'}\n",
            "{'answer': '.\\n\\n'}\n",
            "{'answer': 'If'}\n",
            "{'answer': ' you'}\n",
            "{'answer': \"'re\"}\n",
            "{'answer': ' interested'}\n",
            "{'answer': ' in'}\n",
            "{'answer': ' learning'}\n",
            "{'answer': ' more'}\n",
            "{'answer': ','}\n",
            "{'answer': ' you'}\n",
            "{'answer': ' can'}\n",
            "{'answer': ' explore'}\n",
            "{'answer': ' the'}\n",
            "{'answer': ' User'}\n",
            "{'answer': ' Guide'}\n",
            "{'answer': ' to'}\n",
            "{'answer': ' understand'}\n",
            "{'answer': ' the'}\n",
            "{'answer': ' workflows'}\n",
            "{'answer': ' supported'}\n",
            "{'answer': ' by'}\n",
            "{'answer': ' Lang'}\n",
            "{'answer': 'Smith'}\n",
            "{'answer': ' at'}\n",
            "{'answer': ' each'}\n",
            "{'answer': ' stage'}\n",
            "{'answer': ' of'}\n",
            "{'answer': ' the'}\n",
            "{'answer': ' L'}\n",
            "{'answer': 'LM'}\n",
            "{'answer': ' application'}\n",
            "{'answer': ' lifecycle'}\n",
            "{'answer': ','}\n",
            "{'answer': ' as'}\n",
            "{'answer': ' well'}\n",
            "{'answer': ' as'}\n",
            "{'answer': ' the'}\n",
            "{'answer': ' Setup'}\n",
            "{'answer': ' section'}\n",
            "{'answer': ' to'}\n",
            "{'answer': ' learn'}\n",
            "{'answer': ' how'}\n",
            "{'answer': ' to'}\n",
            "{'answer': ' create'}\n",
            "{'answer': ' an'}\n",
            "{'answer': ' account'}\n",
            "{'answer': ' and'}\n",
            "{'answer': ' obtain'}\n",
            "{'answer': ' an'}\n",
            "{'answer': ' API'}\n",
            "{'answer': ' key'}\n",
            "{'answer': '.'}\n",
            "{'answer': ''}\n"
          ]
        }
      ],
      "source": [
        "stream = conversational_retrieval_chain.stream(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            HumanMessage(content=\"Can LangSmith help test my LLM applications?\"),\n",
        "            AIMessage(\n",
        "                content=\"Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.\"\n",
        "            ),\n",
        "            HumanMessage(content=\"Tell me more!\"),\n",
        "        ],\n",
        "    }\n",
        ")\n",
        "\n",
        "for chunk in stream:\n",
        "    print(chunk)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
