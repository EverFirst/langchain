{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "259M-8OXmQD7"
      },
      "source": [
        "# Multiple chains"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1. 필수 Library 설치\n",
        "LCEL을 사용하기 위한 필수 라이브러리 설치"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "4Vahr4cWmPN6"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --quiet langchain langchain-openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2. OpenAI API KEY 인증"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "86kvSCFwnI2s"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"]      = \"sk-*******************************************************\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3. Multiple Chain 실행"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### 1) Chain 결과값을 활용한 Chain 연결 \n",
        "Chain2(Chain1_Output, Chain2_Value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "G0qTpQ-JB3GQ",
        "outputId": "6bbd2ce9-cf48-47fc-c6cf-0ccb94d5fe7a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'시카고는 일리노이 주에 위치한 도시입니다.'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "#Multi로 들어갈 Prompt Template 생성\n",
        "prompt1 = ChatPromptTemplate.from_template(\"what is the city {person} is from?\")\n",
        "prompt2 = ChatPromptTemplate.from_template(\n",
        "    \"what country is the city {city} in? respond in {language}\"\n",
        ")\n",
        "\n",
        "#OpenAI Model 불러오기\n",
        "model = ChatOpenAI()\n",
        "\n",
        "#LCEL 구조로 Prompt별 Chain 생성\n",
        "#StrOutputParser() : String으로 Output 반환\n",
        "chain1 = prompt1 | model | StrOutputParser()\n",
        "\n",
        "#Chain1 의 결과값을 Value로 설정하여 받아옴.\n",
        "#itemgetter : \"language\" 변수를 받아오도록 설정.\n",
        "chain2 = (\n",
        "    {\"city\": chain1, \"language\": itemgetter(\"language\")}\n",
        "    | prompt2\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "#chain2 실행\n",
        "chain2.invoke({\"person\": \"obama\", \"language\": \"korean\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### 2) Generator 활용 \n",
        "Multi Prompt를 LCEL 형태의 chain으로 만들어 generator에 연결한 후 output을 input값으로 활용.\n",
        "\n",
        "model_parser = model에 프롬프트를 전달 \n",
        "\n",
        "chain 1 = prompt1 | model_parser\n",
        "\n",
        "chain 2 = prompt2 | model_parser...\n",
        "\n",
        "generator = (chain1 | chain2 ...)\n",
        "\n",
        "final_prompt = generator.invoke(\"\") # input 값을 넣어서 generator 결과값 도출 \n",
        "\n",
        "model.invoke(final_prompt) # generator 결과값을 모델에 실행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVuDC5YvINYJ",
        "outputId": "74d27827-614a-48df-cf90-df1f4c0d3f70"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='The color of orange is orange, and the flag of Cyprus is white with a copper-colored map of the island in the center.', response_metadata={'finish_reason': 'stop', 'logprobs': None})"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "#multi prompt 생성 \n",
        "prompt1 = ChatPromptTemplate.from_template(\n",
        "    \"generate a {attribute} color. Return the name of the color and nothing else:\"\n",
        ")\n",
        "prompt2 = ChatPromptTemplate.from_template(\n",
        "    \"what is a fruit of color: {color}. Return the name of the fruit and nothing else:\"\n",
        ")\n",
        "prompt3 = ChatPromptTemplate.from_template(\n",
        "    \"what is a country with a flag that has the color: {color}. Return the name of the country and nothing else:\"\n",
        ")\n",
        "prompt4 = ChatPromptTemplate.from_template(\n",
        "    \"What is the color of {fruit} and the flag of {country}?\"\n",
        ")\n",
        "\n",
        "#LCEL Model parser 생성\n",
        "model_parser = model | StrOutputParser()\n",
        "\n",
        "#각각의 개별적인 chain 생성\n",
        "color_generator = (\n",
        "    {\"attribute\": RunnablePassthrough()} | prompt1 | {\"color\": model_parser}\n",
        ")\n",
        "color_to_fruit = prompt2 | model_parser\n",
        "color_to_country = prompt3 | model_parser\n",
        "\n",
        "#각 chain을 generator로 연결\n",
        "question_generator = (\n",
        "    color_generator | {\"fruit\": color_to_fruit, \"country\": color_to_country} | prompt4\n",
        ")\n",
        "\n",
        "#generator에 input값을 넣어 최종 prompt 생성 \n",
        "prompt = question_generator.invoke(\"warm\")\n",
        "\n",
        "#최종 prompt를 모델에 적용하여 최종 답변 도출 \n",
        "model.invoke(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDPMPwQUI3MY"
      },
      "source": [
        "## Branching and Merging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### LCEL 을 여러개 생성하여 하나의 Chain으로 결합"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "eUH0CknII4c5",
        "outputId": "f8521794-2c11-404f-9a6c-51affb7af908"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'While Scrum offers numerous benefits such as promoting collaboration, adaptability, and continuous improvement, there are indeed some potential drawbacks to consider. These include the need for a high level of commitment from team members, challenges with large or complex projects, potential undefined roles and responsibilities, scope creep risks, and dependencies on strong team dynamics.\\n\\nTo address these concerns and maximize the effectiveness of Scrum, it is crucial to ensure clear communication, well-defined roles, and a strong commitment from all team members. Additionally, proper project planning and monitoring can help mitigate scope creep and adapt Scrum practices to suit the specific needs of the project. By proactively addressing these potential challenges, teams can harness the power of Scrum to deliver high-quality results efficiently and effectively.'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#입력값을 받아 base_response 변수에 저장\n",
        "planner = (\n",
        "    ChatPromptTemplate.from_template(\"Generate an argument about: {input}\")\n",
        "    | ChatOpenAI()\n",
        "    | StrOutputParser()\n",
        "    | {\"base_response\": RunnablePassthrough()}\n",
        ")\n",
        "\n",
        "#planner의 결과값을 받아서 실행\n",
        "arguments_for = (\n",
        "    ChatPromptTemplate.from_template(\n",
        "        \"List the pros or positive aspects of {base_response}\"\n",
        "    )\n",
        "    | ChatOpenAI()\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "#planner의 결과 값을 받아서 실행\n",
        "arguments_against = (\n",
        "    ChatPromptTemplate.from_template(\n",
        "        \"List the cons or negative aspects of {base_response}\"\n",
        "    )\n",
        "    | ChatOpenAI()\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "#모든 결과값을 하나의 답변으로 생성\n",
        "final_responder = (\n",
        "    ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"ai\", \"{original_response}\"),\n",
        "            (\"human\", \"Pros:\\n{results_1}\\n\\nCons:\\n{results_2}\"),\n",
        "            (\"system\", \"Generate a final response given the critique\"),\n",
        "        ]\n",
        "    )\n",
        "    | ChatOpenAI()\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "#실행 순서대로 Chain 생성 \n",
        "# planner 실행 - planner 값을 활용한 결과값을 지정 - final_responder에서 결과 값을 받아와 최종 실행\n",
        "chain = (\n",
        "    planner\n",
        "    | {\n",
        "        \"results_1\": arguments_for,\n",
        "        \"results_2\": arguments_against,\n",
        "        \"original_response\": itemgetter(\"base_response\"),\n",
        "    }\n",
        "    | final_responder\n",
        ")\n",
        "\n",
        "#chain 실행\n",
        "chain.invoke({\"input\": \"scrum\"})"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
